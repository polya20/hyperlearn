{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import from_numpy as toTensor, svd as __svd, qr as __qr, einsum as t_einsum, \\\n",
    "    pinverse as __pinv, div as divide, transpose as __transpose, Tensor as typeTensor, \\\n",
    "    stack as __stack\n",
    "from functools import wraps      \n",
    "from numpy import finfo as np_finfo, einsum as np_einsum\n",
    "from numpy import float32 as np_float32, float64 as np_float64, int32 as np_int32, int64 as np_int64\n",
    "from scipy.linalg.lapack import clapack\n",
    "from scipy.stats import t as tdist\n",
    "use_numpy_einsum = True\n",
    "\n",
    "def Tensor(*args):\n",
    "    out = []\n",
    "    for x in args:\n",
    "        if type(x) is not typeTensor:\n",
    "            out.append(  toTensor(x)  )\n",
    "        else:\n",
    "            out.append(  x  )\n",
    "    return out\n",
    "\n",
    "def Numpy(*args):\n",
    "    out = []\n",
    "    for x in args:\n",
    "        if type(x) is typeTensor:\n",
    "            out.append(  x.numpy()  )\n",
    "        else:\n",
    "            out.append(  x  )\n",
    "    return out\n",
    "\n",
    "def return_numpy(*args):\n",
    "    result = Numpy(*args)\n",
    "    if len(result) == 1:\n",
    "        return result[0]\n",
    "    else:\n",
    "        return tuple(result)\n",
    "\n",
    "def n2n(f):\n",
    "    @wraps(f)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        returned = f(*Tensor(*args), **kwargs)\n",
    "        \n",
    "        if type(returned) not in (tuple, list):\n",
    "            returned = [returned]\n",
    "        \n",
    "        return return_numpy(*returned)\n",
    "    return wrapper\n",
    "\n",
    "def n2t(f):\n",
    "    @wraps(f)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        returned = f(*Tensor(*args), **kwargs)\n",
    "        return returned\n",
    "    return wrapper\n",
    "\n",
    "def einsum(notation, *args, tensor = False):\n",
    "    if use_numpy_einsum:\n",
    "        args = Numpy(*args)\n",
    "        out = np_einsum(notation, *args)\n",
    "    else:\n",
    "        args = Tensor(*args)\n",
    "        try:\n",
    "            out = t_einsum(notation, *args)\n",
    "        except:\n",
    "            out = t_einsum(notation, args)\n",
    "    if tensor:\n",
    "        return toTensor(out)\n",
    "    return out\n",
    "\n",
    "def T(X):\n",
    "    if type(X) is not typeTensor:\n",
    "        X = toTensor(X)\n",
    "    if len(X.shape) == 1:\n",
    "        return X.reshape(-1,1)\n",
    "    return __transpose(X, 0, 1)\n",
    "\n",
    "def ravel(y, X):\n",
    "    return Tensor(y.ravel().astype( dtype(X) ))\n",
    "\n",
    "def constant(X):\n",
    "    return X.item()\n",
    "\n",
    "def eps(X):\n",
    "    try:\n",
    "        return np_finfo(dtype(X)).eps\n",
    "    except:\n",
    "        return np_finfo(np_float64).eps\n",
    "    \n",
    "def dtype(tensor):\n",
    "    if 'float32' in str(tensor.dtype): return np_float32\n",
    "    elif 'float64' in str(tensor.dtype): return np_float64\n",
    "    elif 'int32' in str(tensor.dtype): return np_int32\n",
    "    elif 'int64' in str(tensor.dtype): return np_int64\n",
    "    else: return np_float32\n",
    "    \n",
    "def stack(*args):\n",
    "    if type(args[0]) is list:\n",
    "        args = args[0]\n",
    "    toStack = Tensor(*args)\n",
    "    return __stack(toStack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_svd(X):\n",
    "    U, S, V = __svd(X, some = True)\n",
    "    return U, S, T(V)\n",
    "svd = n2n(t_svd)\n",
    "_svd = n2t(t_svd)\n",
    "\n",
    "\n",
    "def t_pinv(X):\n",
    "    U, S, VT = _svd(X)\n",
    "    cond = S < eps(X)*constant(S[0])\n",
    "    _S = 1.0 / S\n",
    "    _S[cond] = 0.0\n",
    "    VT *= T(_S)\n",
    "    return T(VT).matmul(T(U))\n",
    "pinv = n2n(t_pinv)\n",
    "_pinv = n2t(t_pinv)\n",
    "\n",
    "\n",
    "def t_qr(X):\n",
    "    return __qr(X)\n",
    "qr = n2n(t_qr)\n",
    "_qr = n2t(t_qr)\n",
    "\n",
    "\n",
    "def qr_solve(X, y):\n",
    "    '''\n",
    "    theta =  R^-1 * QT * y\n",
    "    '''\n",
    "    Q, R = qr(X)\n",
    "    check = 0\n",
    "    if R.shape[0] == R.shape[1]:\n",
    "        _R, check = clapack.strtri(R)\n",
    "    if check > 0:\n",
    "        _R = _pinv(R)\n",
    "    Q, _R, R = Tensor(Q, _R, R)\n",
    "    \n",
    "    theta_hat = _R.matmul(   T(Q).matmul( ravel(y, Q) )   )\n",
    "    return theta_hat\n",
    "\n",
    "\n",
    "def svd_solve(X, y):\n",
    "    '''\n",
    "    theta =  V * S^-1 * UT * y\n",
    "    '''\n",
    "    U, S, VT = _svd(X)\n",
    "    cond = S < eps(X)*constant(S[0])\n",
    "    _S = 1.0 / S\n",
    "    _S[cond] = 0.0\n",
    "    VT *= T(_S)\n",
    "    \n",
    "    theta_hat = T(VT).matmul(  \n",
    "                            T(U).matmul(  ravel(y, U)  )\n",
    "                            )\n",
    "    return theta_hat\n",
    "\n",
    "\n",
    "def ridge_solve(X, y, alpha = 1):\n",
    "    '''\n",
    "                    S\n",
    "    theta =   V --------- UT y \n",
    "                 S^2 + aI\n",
    "    '''\n",
    "    U, S, VT = _svd(X)\n",
    "    cond = S < eps(X)*constant(S[0])\n",
    "    _S = S / (S**2 + alpha)\n",
    "    _S[cond] = 0.0\n",
    "    VT *= T(_S)\n",
    "    \n",
    "    theta_hat = T(VT).matmul(  \n",
    "                            T(U).matmul(  ravel(y, U)  )\n",
    "                            )\n",
    "    return theta_hat\n",
    "\n",
    "\n",
    "def qr_stats(Q, R):\n",
    "    '''\n",
    "    XTX^-1  =  RT * R\n",
    "    \n",
    "    h = diag  Q * QT\n",
    "    \n",
    "    mean(h) used for normalized leverage\n",
    "    '''\n",
    "    XTX = T(R).matmul(R)\n",
    "    _XTX = pinv(XTX)\n",
    "    ## Einsum is slow in pytorch so revert to numpy version\n",
    "    h = einsum('ij,ij->i', Q, Q )\n",
    "    h_mean = h.mean()\n",
    "    \n",
    "    return _XTX, h, h_mean\n",
    "\n",
    "\n",
    "def svd_stats(U, S, VT):\n",
    "    '''\n",
    "                  1\n",
    "    XTX^-1 =  V ----- VT \n",
    "                 S^2\n",
    "    \n",
    "    h = diag U * UT\n",
    "    \n",
    "    mean(h) used for normalized leverage\n",
    "    '''\n",
    "    _S2 = 1.0 / (S**2)\n",
    "    VS = T(VT) * _S2\n",
    "    _XTX = VS.matmul(VT)\n",
    "    h = einsum('ij,ij->i', U, U )\n",
    "    h_mean = h.mean()\n",
    "    \n",
    "    return _XTX, h, h_mean\n",
    "\n",
    "\n",
    "def ridge_stats(U, S, VT, alpha = 1):\n",
    "    '''\n",
    "                               S^2\n",
    "    exp_theta_hat =  diag V --------- VT\n",
    "                            S^2 + aI\n",
    "                            \n",
    "                                 S^2\n",
    "    var_theta_hat =  diag V ------------- VT\n",
    "                            (S^2 + aI)^2\n",
    "    \n",
    "                    1\n",
    "    XTX^-1 =  V --------- VT\n",
    "                S^2 + aI\n",
    "                \n",
    "                  S^2\n",
    "    h = diag U --------- UT\n",
    "                S^2 + aI\n",
    "    \n",
    "    mean(h) used for normalized leverage\n",
    "    '''\n",
    "    V = T(VT)\n",
    "    S2 = S**2\n",
    "    S2_alpha = S2 + alpha\n",
    "    S2_over_S2 = S2 / S2_alpha\n",
    "    \n",
    "    VS = V * S2_over_S2\n",
    "    exp_theta_hat = einsum('ij,ji->i', VS, VT )     # Same as VS.dot(VT)\n",
    "    \n",
    "    V_S2 = VS / S2_alpha  # np_divide(S2,  np_square( S2 + alpha ) )\n",
    "    var_theta_hat = einsum('ij,ij->i',  V_S2  , V )   # Sams as np_multiply(   V,   V_S2 ).sum(1)\n",
    "    \n",
    "    _XTX = (V * (1.0 / S2_alpha )  ).matmul( VT )   # V  1/S^2 + a  VT\n",
    "    \n",
    "    h = einsum('ij,ij->i', (U * S2_over_S2), U )  # Same as np_multiply(  U*S2_over_S2, U ).sum(1)\n",
    "    h_mean = h.mean()\n",
    "    \n",
    "    return exp_theta_hat, var_theta_hat, _XTX, h_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hyperlearn.hyperlearn.linalg import *\n",
    "from hyperlearn.hyperlearn.solvers import *\n",
    "from hyperlearn.hyperlearn.base import *\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "import torch\n",
    "import scipy.linalg as linalg\n",
    "n = 100000\n",
    "p = 200\n",
    "#X, y = make_classification(random_state = 0, n_samples = int(n/2), n_features = p, n_classes = 10,\n",
    "#                          n_informative = int(p/3))\n",
    "\n",
    "X, y = make_regression(random_state = 0, n_samples = n, n_features = p, n_informative = int(p/3))\n",
    "\n",
    "X[0,0] = 0.0\n",
    "X[1,1] = 1.0\n",
    "X[:int(n/2),2] = 0\n",
    "X[int(n/2):,2] = 1\n",
    "X[:int(n/2),3] = 0\n",
    "X[int(n/2):,3] = 1\n",
    "X[:int(n/2),4] = 1\n",
    "X[int(n/2):,4] = 0\n",
    "\n",
    "for x in range(10,50):\n",
    "    X[:int(n/2),x] = 1\n",
    "    X[int(n/2):,x] = 2\n",
    "    \n",
    "X = X.astype(np.float32, copy = False)\n",
    "y = y.astype(np.float32, copy = False)\n",
    "results = [n,p]\n",
    "# y = y.astype(np.str)\n",
    "\n",
    "def eigh(X):\n",
    "    return linalg.eigh(X, b = np.eye(len(X), dtype = X.dtype), turbo = True,\n",
    "                 check_finite = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S ,VT = linalg.svd(X, full_matrices = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1., -0., -0., ...,  0., -0., -0.],\n",
       "       [-0., -0., -0., ...,  0.,  0., -0.],\n",
       "       [-0., -0., -0., ...,  0.,  0.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  0., ..., -1.,  0., -0.],\n",
       "       [-0.,  0.,  0., ...,  0., -1.,  0.],\n",
       "       [-0., -0.,  0., ..., -0.,  0., -1.]], dtype=float32)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(eigh(XTX)[1][:,::-1] @ VT).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (200,3) and (200,3) not aligned: 3 (dim 1) != 200 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-323-0a70c73d0072>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0meigsh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvds\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0meigsh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXTX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhich\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'LM'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0msvds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhich\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'LM'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: shapes (200,3) and (200,3) not aligned: 3 (dim 1) != 200 (dim 0)"
     ]
    }
   ],
   "source": [
    "from scipy.sparse.linalg import eigsh, svds\n",
    "eigsh(A = XTX, k = 3, which = 'LM')[1] @ svds(A = X, k = 3, which = 'LM')[2].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import fbpca \n",
    "from scipy.sparse.linalg import svds\n",
    "print(round(fbpca.diffsnorm(X, *svds(X, k = 6)), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# round(fbpca.diffsnorm(X, *np.linalg.svd(X, full_matrices = False)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(round(fbpca.diffsnorm(X, *fbpca.pca(X, k = 6)), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X.dot(choleskySolve(X, y, alpha = 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept = False, normalize = False, n_jobs = -1)\n",
    "model.fit(X, y)\n",
    "squareSum(y - model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "from tensorflow.linalg import lstsq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004827002\n",
      "Wall time: 921 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "soln = torch.gels(Tensor(y).reshape(-1,1), Tensor(X))[0][:X.shape[1]].flatten()\n",
    "print(squareSum(y - X @ soln))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q, R = linalg.qr(X.T.dot(X) + diagonal(p, 1, X.dtype), mode = 'economic', check_finite = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preds = X @ (dtrtri(R)[0] @ (Q.T @ (X.T @ y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dtrtri(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squareSum(y - X @ choleskySolve(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg.lapack import dtrtri\n",
    "np.set_printoptions(suppress = True, floatmode = 'fixed', precision = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "einsum('i,j->', dtrtri(R)[0].dot(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "linalg.solve_triangular(R, dot(Q.T, XTy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagSum(Tensor(dtrtri(R)[0]), Tensor(R), transpose_a = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "einsum('ji,ij->', dtrtri(R)[0] , R )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "j = Tensor(X).t()\n",
    "j @= Tensor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTX = cov(X)\n",
    "XTy = X.T @ y\n",
    "Q, R = linalg.qr(XTX + diagonal(p, 1, X.dtype) , mode ='economic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((y - X.dot(choleskySolve(X, y)))**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "alpha = 0\n",
    "use_gpu = True\n",
    "XTX = cov(X)\n",
    "p = XTX.shape[0]\n",
    "alpha = np_finfo(X.dtype).resolution if alpha == 0 else alpha\n",
    "regularizer = diagonal(p, 1, X.dtype)\n",
    "\n",
    "no_success = True\n",
    "warn = False\n",
    "\n",
    "while no_success:\n",
    "    alphaI = regularizer*alpha\n",
    "    try:\n",
    "        if use_gpu: chol = cholesky(  XTX + alphaI  )\n",
    "        else: chol = linalg.cholesky(  XTX + alphaI  , check_finite = False)\n",
    "        no_success = False\n",
    "    except:\n",
    "        alpha *= 10\n",
    "        print(alpha)\n",
    "        warn = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import scipy.linalg as linalg\n",
    "U, S, VT = linalg.svd(X, full_matrices = False, check_finite = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.round(2)[:10], Sa.round(2)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.dot(np.diag(Sa)).dot(VTa).round(2), X.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "U2, S2, VT2 = linalg.svd(X.T.dot(X), full_matrices = True, check_finite = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VT2.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lambda_, V = linalg.eigh(X.T.dot(X), check_finite  = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, R = torch.qr(Tensor(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "U2, S2, VT2 = torch.svd(Tensor(X.T.dot(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inv = torch.pinverse(Tensor(X.T.dot(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "inv = linalg.pinvh(X.T.dot(X) + np.diag(np.ones(p)*0.001), check_finite = False, return_rank = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linalg.svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 276 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from numpy.linalg import svd as np_svd, lstsq as np_lstsq, qr as np_qr, pinv as np_pinv \\\n",
    "                    , eigh as np_eigh\n",
    "from numpy import finfo as np_finfo, divide as np_divide, dot as np_dot, multiply as np_multiply, \\\n",
    "                    einsum, square as np_square, newaxis as np_newaxis, log as np_log, sqrt as np_sqrt, \\\n",
    "                    arange as np_arange, array as np_array, argmax as np_argmax, sign as np_sign, \\\n",
    "                    abs as np_abs\n",
    "from numba.types import Tuple as _Tuple, float32, float64, int32, int64, Array, UniTuple\n",
    "from numba import njit, jit\n",
    "from scipy.stats import t as tdist\n",
    "import numpy as np\n",
    "\n",
    "matrix32 = float32[:,:]\n",
    "matrix64 = float64[:,:]\n",
    "array32 = float32[:]\n",
    "array64 = float64[:]\n",
    "int32A = int32[:]\n",
    "int64A = int64[:]\n",
    "\n",
    "def column(a): return a[:,np_newaxis]\n",
    "def row(a): return a[np_newaxism,:]\n",
    "\n",
    "# @njit( [ Tuple((matrix32, array32, matrix32))  (matrix32) ,\n",
    "#          Tuple((matrix64, array64, matrix64))  (matrix64) ] , fastmath = True, nogil = True)\n",
    "# def ___svd(X):\n",
    "#     return np_svd(X, full_matrices = False)\n",
    "\n",
    "# @njit( [ UniTuple(matrix32, 2)  (matrix32) ,\n",
    "#          UniTuple(matrix64, 2)  (matrix64) ] , fastmath = True, nogil = True)\n",
    "# def _qr(X):\n",
    "#     return np_qr(X)\n",
    "\n",
    "# @njit( [ matrix32(matrix32) , matrix64(matrix64) ] , fastmath = True, nogil = True)\n",
    "# def ___pinv(X):\n",
    "#     U, S, VT = np_svd(X, full_matrices = False)\n",
    "#     cond =  S < np_finfo(X.dtype).eps*S[0] \n",
    "#     S = 1/S\n",
    "#     S[cond] = 0.0\n",
    "#     VT *= S.reshape(-1,1)\n",
    "#     return VT.T @ U.T\n",
    "\n",
    "# @njit( [ matrix32(matrix32) , matrix64(matrix64) ] , fastmath = True, nogil = True)\n",
    "# def __pinv(X):\n",
    "#     return np_pinv(X)\n",
    "\n",
    "\n",
    "from numba import f4, f8\n",
    "\n",
    "def Tuple(*args):\n",
    "    return _Tuple(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@njit(['f4[:,:](f4[:,:])','f8[:,:](f8[:,:])'] , fastmath = True, nogil = True, cache = True)\n",
    "def ___pinv(X):\n",
    "    U, S, VT = np_svd(X, full_matrices = False)\n",
    "    cond =  S < np_finfo(X.dtype).eps*S[0]\n",
    "    S = 1/S\n",
    "    S[cond] = 0.0\n",
    "    VT *= S.reshape(-1,1)\n",
    "    return VT.T @ U.T\n",
    "\n",
    "\n",
    "# @njit( ['f4[:](f4[:,:], f4[:])','f8[:](f8[:,:], f8[:])'] , fastmath = True, nogil = True, cache = True)\n",
    "# def ___lstsq(X, y):\n",
    "#     return np_lstsq(X, y.astype(X.dtype))[0]\n",
    "\n",
    "\n",
    "# @njit( [ Tuple((f4[:], f4[:,:]))(f4[:,:]) , Tuple((f8[:], f8[:,:]))(f8[:,:]) ] , fastmath = True, nogil = True)\n",
    "# def sigmaV(X):\n",
    "#     S, V = np_eigh(X)\n",
    "#     S[S < 0] = 0.0\n",
    "#     S **= 0.5\n",
    "\n",
    "#     S = S[::-1]\n",
    "#     VT = V[:,::-1].T\n",
    "\n",
    "#     return S, VT\n",
    "\n",
    "\n",
    "@njit( [ Tuple(f8[:], f8[:,:])(f8[:,:]), Tuple(f4[:], f4[:,:])(f4[:,:]) ] , fastmath = True, nogil = True)\n",
    "def eigh_pinv(X):\n",
    "    S2, V = np_eigh(X)\n",
    "    return S2, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206.06060861065558"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "XT = X.T\n",
    "XTX = XT @ X\n",
    "XTX.flat[::XTX.shape[0]+1] += 0.01\n",
    "cho = linalg.cholesky(  XTX  , check_finite = False )\n",
    "\n",
    "from scipy.linalg.lapack import dtrtri, strtri\n",
    "_cho = dtrtri(cho)[0]\n",
    "_XTX = _cho @ _cho.T\n",
    "inv = _XTX @ XT\n",
    "(inv @ X).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 200)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "217190.66715885705\n",
      "199.99681909288105\n"
     ]
    }
   ],
   "source": [
    "## %%time\n",
    "XT = X.T\n",
    "XTX = XT @ X\n",
    "XTX.flat[::XTX.shape[0]+1] += 0.001\n",
    "S2, V = eigh(XTX + np.eye(p))\n",
    "cond = np.finfo(X.dtype).eps**2*S2[-1]\n",
    "# S2[S2 < cond] = cond\n",
    "V /= S2\n",
    "inv = (V @ V.T) @ XT\n",
    "VT = V[:,::-1].T\n",
    "S2 = S2[::-1]\n",
    "V = VT.T\n",
    "cond = S2 < 0\n",
    "S2[cond] = 1\n",
    "S = S2 ** 0.5\n",
    "S2[cond] = np.finfo(X.dtype).eps*S[0]\n",
    "\n",
    "# U = (X @ VT.T) / S\n",
    "# pseudo = (V / S) @ U.T\n",
    "print(np.abs(inv @ X).round(3).sum())\n",
    "print(np.abs(inv @ X).diagonal().round(3).sum())\n",
    "print(squareSum(y - X @ (inv @ y))/n)\n",
    "print(np.square(np.eye(p) - inv @ X).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "XTX = X.T @ X\n",
    "inv = linalg.eigh(XTX, b = np.eye(len(XTX), dtype = X.dtype), turbo = True,\n",
    "                 check_finite = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "XTX = X.T @ X\n",
    "inv = eigh_pinv(XTX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inv = ___pinv(X)\n",
    "print(np.abs(inv @ X).round(3).sum())\n",
    "print(np.abs(inv @ X).diagonal().round(3).sum())\n",
    "print(squareSum(y - X @ (inv @ y))/n)\n",
    "print(np.square(np.eye(p) - inv @ X).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inv = linalg.pinv2(X, check_finite = False, return_rank = False)\n",
    "print(np.abs(inv @ X).round(3).sum())\n",
    "print(np.abs(inv @ X).diagonal().round(3).sum())\n",
    "print(squareSum(y - X @ (inv @ y))/n)\n",
    "print(np.square(np.eye(p) - inv @ X).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "import numpy as np\n",
    "X = np.array([[1,2,np.nan,4],\n",
    "         [5,np.nan,7,8],\n",
    "         [9,10,np.nan,np.nan]]).T\n",
    "mask = np.isnan(X)\n",
    "X[mask] = 0\n",
    "XTX = X.T @ X\n",
    "row_mask = np.ones(len(XTX), dtype = bool)\n",
    "col_mask = np.ones(len(XTX), dtype = bool)\n",
    "\n",
    "row_mask[0] = 0\n",
    "col_mask.ravel()[0] = 0\n",
    "locate = np.ones((len(XTX), len(XTX)), dtype = bool)\n",
    "locate[0] = 0\n",
    "locate[:,0] = 0\n",
    "XTX[locate].reshape((len(XTX)-1,len(XTX)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = X.mean(0)\n",
    "U2, S22, VT2 = linalg.svd(X, full_matrices = False)\n",
    "# from sklearn.utils.extmath import svd_flip\n",
    "# U2, VT2 = svd_flip(U2, VT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((((X @ VT.T) / S) * S) @ VT).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((U2 * S2) @ VT2).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = (U * S) @ VT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "XXT = X @ X.T\n",
    "XXT.flat[::XXT.shape[0]+1] += 0.001\n",
    "chol = linalg.cho_factor(  XXT  , check_finite = False)\n",
    "t = X.T @ linalg.cho_solve(chol, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4453757746.687452\n",
      "Wall time: 251 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "XT = X.T\n",
    "XTX = XT @ X\n",
    "# XTX.flat[::XTX.shape[0]+1] \n",
    "cho = linalg.cholesky(  XTX + np.eye(p)*0.001  , check_finite = False )\n",
    "t = linalg.cho_solve((cho, False), XT @ y)\n",
    "print(squareSum(y - X @ t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "XT = X.T\n",
    "XTX = XT @ X\n",
    "# XTX.flat[::XTX.shape[0]+1] \n",
    "cho = linalg.cholesky(  XTX + np.eye(p)*0.001  , check_finite = False )\n",
    "from scipy.linalg.lapack import dtrtri, strtri\n",
    "_cho = dtrtri(cho)[0]\n",
    "_XTXa = _cho @ _cho.T\n",
    "t = _XTXa @ (XT @ y)\n",
    "sigma2 = squareSum(y - X @ t)\n",
    "sigma = sigma2 ** 0.5\n",
    "\n",
    "_XTXaXTX = _XTXa @ XTX\n",
    "exp_value = _XTXaXTX.diagonal()\n",
    "variance = np.einsum('ij,ij->i', _XTXaXTX, _XTXa)\n",
    "cond = variance < 0\n",
    "variance[cond] = variance[~cond].max() * p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199.99648"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.einsum('ij,ij->i', U, U).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.00487580e-05,  1.00161074e-05,  1.12787099e-05, -1.04842211e-05,\n",
       "        1.48928820e-05,  1.00634933e-05,  9.98782161e-06,  1.00052212e-05,\n",
       "        1.00129233e-05,  9.96957780e-06, -6.65776243e-08, -2.91711392e-07,\n",
       "        1.60480360e-07, -6.51540595e-07, -2.55151228e-07, -4.34103714e-07,\n",
       "       -1.16338318e-06, -4.41800586e-07, -4.14861511e-07,  2.02813210e-07,\n",
       "       -4.39876355e-07, -1.68561305e-07, -1.93576166e-07, -4.53345913e-07,\n",
       "        1.85495209e-07, -1.37773788e-07,  2.77857788e-07,  9.89053121e-08,\n",
       "       -5.69565294e-08, -2.55151217e-07, -3.78301304e-07, -1.07752337e-08,\n",
       "        1.08526423e-07,  1.77798338e-07,  1.89343660e-07, -5.11838701e-08,\n",
       "        1.08526416e-07,  3.85614128e-07,  4.20250107e-07, -2.76317636e-07,\n",
       "       -3.28271567e-07, -2.22439455e-07, -4.14861482e-07, -7.65069579e-07,\n",
       "       -8.47811058e-07, -7.51600047e-07, -1.07486904e-06, -1.29230591e-06,\n",
       "       -1.05177806e-06, -2.58345693e-06,  9.97577870e-06,  9.90624514e-06,\n",
       "        9.98797963e-06,  1.01056829e-05,  1.00312640e-05,  1.00889321e-05,\n",
       "        1.00318159e-05,  9.97072930e-06,  9.96166960e-06,  9.94697026e-06,\n",
       "        1.00241962e-05,  9.99421313e-06,  1.00556975e-05,  1.00236651e-05,\n",
       "        1.00109595e-05,  1.00076551e-05,  9.99820566e-06,  9.90671151e-06,\n",
       "        1.00482188e-05,  1.00506338e-05,  1.00010507e-05,  1.00416951e-05,\n",
       "        1.00768306e-05,  1.00214040e-05,  1.00273969e-05,  1.00166813e-05,\n",
       "        9.96768004e-06,  9.96875587e-06,  1.00534064e-05,  1.00925550e-05,\n",
       "        1.00850988e-05,  9.97441835e-06,  9.96059389e-06,  1.00429702e-05,\n",
       "        9.97870863e-06,  1.00520987e-05,  1.01172825e-05,  1.01227846e-05,\n",
       "        9.97838466e-06,  9.92806424e-06,  1.00481681e-05,  1.01098504e-05,\n",
       "        1.00366513e-05,  1.00389692e-05,  1.00414975e-05,  1.00344681e-05,\n",
       "        1.00514750e-05,  1.00273043e-05,  9.97923202e-06,  1.00820512e-05,\n",
       "        1.01839894e-05,  1.00334866e-05,  9.99827549e-06,  1.00395042e-05,\n",
       "        9.96132179e-06,  1.00565273e-05,  1.00199104e-05,  1.00096744e-05,\n",
       "        9.98497340e-06,  1.00186068e-05,  1.00568062e-05,  1.00361863e-05,\n",
       "        9.94829933e-06,  1.00771500e-05,  1.01112421e-05,  1.00491131e-05,\n",
       "        1.00405480e-05,  1.00268772e-05,  1.00434454e-05,  1.00975784e-05,\n",
       "        9.99380721e-06,  1.00100636e-05,  9.98152070e-06,  1.00072575e-05,\n",
       "        1.01000999e-05,  9.95210943e-06,  1.00587397e-05,  1.00162294e-05,\n",
       "        1.00910491e-05,  9.96559798e-06,  1.00522065e-05,  9.99712967e-06,\n",
       "        1.00048600e-05,  1.00538586e-05,  1.00414301e-05,  1.01084608e-05,\n",
       "        1.00194031e-05,  1.00242552e-05,  1.00055607e-05,  9.96429849e-06,\n",
       "        1.00261139e-05,  9.97755080e-06,  1.00172168e-05,  9.97381165e-06,\n",
       "        1.00432144e-05,  1.00220317e-05,  9.97247522e-06,  1.00496900e-05,\n",
       "        9.97928130e-06,  1.00150797e-05,  9.98589052e-06,  9.99215679e-06,\n",
       "        9.97073832e-06,  9.97793458e-06,  1.00261005e-05,  1.00548344e-05,\n",
       "        9.99691291e-06,  1.00748997e-05,  9.95074992e-06,  9.95785558e-06,\n",
       "        1.00147520e-05,  1.00632724e-05,  1.00745157e-05,  1.00312829e-05,\n",
       "        1.00632137e-05,  1.00249553e-05,  1.00172367e-05,  1.00263771e-05,\n",
       "        9.99198927e-06,  9.94512813e-06,  1.00197524e-05,  1.00233093e-05,\n",
       "        1.00349424e-05,  9.98883318e-06,  1.00893370e-05,  1.00588781e-05,\n",
       "        1.00162440e-05,  1.00457028e-05,  1.00283352e-05,  9.93350366e-06,\n",
       "        9.99822894e-06,  1.00369703e-05,  1.00045206e-05,  1.00852293e-05,\n",
       "        9.99397369e-06,  1.00715206e-05,  1.00347584e-05,  1.00723379e-05,\n",
       "        1.00965894e-05,  9.94101485e-06,  1.01084686e-05,  1.00478188e-05,\n",
       "        1.00899553e-05,  9.99448777e-06,  1.00931698e-05,  1.00223921e-05,\n",
       "        9.99595274e-06,  1.00358411e-05,  9.98430953e-06,  1.00205035e-05])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(_XTXaXTX @ _XTXa).diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, VT = linalg.svd(X, full_matrices = False, check_finite = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20.98770</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>945.45456</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>12412.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>21.98483</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>963.41464</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>9939.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>21.98483</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>957.14287</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>10141.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>26.73423</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>700.00001</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>27164.001953</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>28.13980</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>958.90412</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>18950.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>28.74244</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>934.78262</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>9956.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>29.03907</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>960.52633</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>9876.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>30.05423</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>903.22582</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>10345.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>35.17788</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>942.30770</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>10469.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>41.44151</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>964.70589</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>10481.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>43.26266</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>965.90910</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>9781.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>259.09481</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>259.000000</td>\n",
       "      <td>49.04716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>209.00711</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>209.000000</td>\n",
       "      <td>-1.04549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>285.98001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>286.000000</td>\n",
       "      <td>75.70117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>210.49519</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>0.15875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>269.93510</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>59.51915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>210.47997</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>0.02050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>210.25495</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>-0.22402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>238.15485</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>27.66182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>210.59933</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>0.08038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>210.10279</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>-0.43054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>210.00307</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>-0.59103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>210.75229</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>0.12924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>210.82658</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>0.19583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>211.71451</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>212.000000</td>\n",
       "      <td>1.08008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>210.86086</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>0.19864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>265.30913</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>265.000000</td>\n",
       "      <td>54.63317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>209.69195</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>-1.00602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>211.68279</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>212.000000</td>\n",
       "      <td>0.97345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>210.77628</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>0.05825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>250.00001</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>18468.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>880.00001</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>11164.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>571.42858</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>18289.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>769.23078</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>22258.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>812.50001</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>18389.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3642.33382</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>500.00001</td>\n",
       "      <td>0.16942</td>\n",
       "      <td>67078.000000</td>\n",
       "      <td>0.59809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>842.10527</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>20981.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>863.63637</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>15189.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>892.85715</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>10441.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>973.91305</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>9714.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>918.91893</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>14215.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>973.21430</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>9068.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>975.20667</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>10150.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>972.47707</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>10547.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>971.69812</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>8446.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>970.87380</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>9766.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>970.00001</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>10133.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>969.07218</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>10505.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>968.08512</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>9380.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>967.03298</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>9664.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>962.02533</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>9527.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>955.22389</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>10688.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>953.12501</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>9905.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>950.81968</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>14553.999023</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>948.27587</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>17277.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>938.77552</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>9980.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>930.23257</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>31441.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>974.57634</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>10250.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>911.76472</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>12765.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3642.23209</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>925.00001</td>\n",
       "      <td>0.02479</td>\n",
       "      <td>10725.000000</td>\n",
       "      <td>-0.01622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0        1          2        3             4         5\n",
       "27     20.98770  0.00000  945.45456  0.02479  12412.000000  -0.01622\n",
       "36     21.98483  0.00000  963.41464  0.02479   9939.000000  -0.01622\n",
       "32     21.98483  0.00000  957.14287  0.02479  10141.000000  -0.01622\n",
       "12     26.73423  0.00000  700.00001  0.02479  27164.001953  -0.01622\n",
       "33     28.13980  0.00000  958.90412  0.02479  18950.000000  -0.01622\n",
       "24     28.74244  0.00000  934.78262  0.02479   9956.000000  -0.01622\n",
       "34     29.03907  0.00000  960.52633  0.02479   9876.000000  -0.01622\n",
       "19     30.05423  0.00000  903.22582  0.02479  10345.000000  -0.01622\n",
       "26     35.17788  0.00000  942.30770  0.02479  10469.000000  -0.01622\n",
       "37     41.44151  0.00000  964.70589  0.02479  10481.000000  -0.01622\n",
       "38     43.26266  0.00000  965.90910  0.02479   9781.000000  -0.01622\n",
       "51    259.09481  0.00001    0.00001  1.00000    259.000000  49.04716\n",
       "67    209.00711  0.00001    0.00001  1.00000    209.000000  -1.04549\n",
       "89    285.98001  0.00001    0.00001  1.00000    286.000000  75.70117\n",
       "179   210.49519  0.00001    0.00001  1.00000    210.000000   0.15875\n",
       "189   269.93510  0.00001    0.00001  1.00000    270.000000  59.51915\n",
       "169   210.47997  0.00001    0.00001  1.00000    210.000000   0.02050\n",
       "59    210.25495  0.00001    0.00001  1.00000    210.000000  -0.22402\n",
       "112   238.15485  0.00001    0.00001  1.00000    238.000000  27.66182\n",
       "158   210.59933  0.00001    0.00001  1.00000    211.000000   0.08038\n",
       "125   210.10279  0.00001    0.00001  1.00000    210.000000  -0.43054\n",
       "159   210.00307  0.00001    0.00001  1.00000    210.000000  -0.59103\n",
       "82    210.75229  0.00001    0.00001  1.00000    211.000000   0.12924\n",
       "104   210.82658  0.00001    0.00001  1.00000    211.000000   0.19583\n",
       "58    211.71451  0.00001    0.00001  1.00000    212.000000   1.08008\n",
       "139   210.86086  0.00001    0.00001  1.00000    211.000000   0.19864\n",
       "129   265.30913  0.00001    0.00001  1.00000    265.000000  54.63317\n",
       "76    209.69195  0.00001    0.00001  1.00000    210.000000  -1.00602\n",
       "77    211.68279  0.00001    0.00001  1.00000    212.000000   0.97345\n",
       "9     210.77628  0.00001    0.00001  1.00000    211.000000   0.05825\n",
       "..          ...      ...        ...      ...           ...       ...\n",
       "10   3642.23209  0.00298  250.00001  0.02479  18468.000000  -0.01622\n",
       "17   3642.23209  0.00298  880.00001  0.02479  11164.000000  -0.01622\n",
       "11   3642.23209  0.00298  571.42858  0.02479  18289.000000  -0.01622\n",
       "13   3642.23209  0.00298  769.23078  0.02479  22258.000000  -0.01622\n",
       "14   3642.23209  0.00298  812.50001  0.02479  18389.000000  -0.01622\n",
       "3    3642.33382  0.00298  500.00001  0.16942  67078.000000   0.59809\n",
       "15   3642.23209  0.00298  842.10527  0.02479  20981.000000  -0.01622\n",
       "16   3642.23209  0.00298  863.63637  0.02479  15189.000000  -0.01622\n",
       "18   3642.23209  0.00298  892.85715  0.02479  10441.000000  -0.01622\n",
       "47   3642.23209  0.00298  973.91305  0.02479   9714.000000  -0.01622\n",
       "21   3642.23209  0.00298  918.91893  0.02479  14215.000000  -0.01622\n",
       "46   3642.23209  0.00298  973.21430  0.02479   9068.000000  -0.01622\n",
       "49   3642.23209  0.00298  975.20667  0.02479  10150.000000  -0.01622\n",
       "45   3642.23209  0.00298  972.47707  0.02479  10547.000000  -0.01622\n",
       "44   3642.23209  0.00298  971.69812  0.02479   8446.000000  -0.01622\n",
       "43   3642.23209  0.00298  970.87380  0.02479   9766.000000  -0.01622\n",
       "42   3642.23209  0.00298  970.00001  0.02479  10133.000000  -0.01622\n",
       "41   3642.23209  0.00298  969.07218  0.02479  10505.000000  -0.01622\n",
       "40   3642.23209  0.00298  968.08512  0.02479   9380.000000  -0.01622\n",
       "39   3642.23209  0.00298  967.03298  0.02479   9664.000000  -0.01622\n",
       "35   3642.23209  0.00298  962.02533  0.02479   9527.000000  -0.01622\n",
       "31   3642.23209  0.00298  955.22389  0.02479  10688.000000  -0.01622\n",
       "30   3642.23209  0.00298  953.12501  0.02479   9905.000000  -0.01622\n",
       "29   3642.23209  0.00298  950.81968  0.02479  14553.999023  -0.01622\n",
       "28   3642.23209  0.00298  948.27587  0.02479  17277.000000  -0.01622\n",
       "25   3642.23209  0.00298  938.77552  0.02479   9980.000000  -0.01622\n",
       "23   3642.23209  0.00298  930.23257  0.02479  31441.000000  -0.01622\n",
       "48   3642.23209  0.00298  974.57634  0.02479  10250.000000  -0.01622\n",
       "20   3642.23209  0.00298  911.76472  0.02479  12765.000000  -0.01622\n",
       "22   3642.23209  0.00298  925.00001  0.02479  10725.000000  -0.01622\n",
       "\n",
       "[200 rows x 6 columns]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([\n",
    "    pd.Series(exp_value*t + sigma*1*(variance**0.5)),\n",
    "    pd.Series(variance),\n",
    "    pd.Series(exp_value),\n",
    "    pd.Series(np.rint(exp*theta + sigma*1*(var ** 0.5))),\n",
    "    pd.Series(t)\n",
    "], 1).sort_values(1).round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp = ((VT.T * (S**2 / (S**2 + 0.001))) @ VT).diagonal()\n",
    "var = ((VT.T * (S**2 / (S**2 + 0.001)**2)) @ VT).diagonal()\n",
    "theta = (VT.T * (S / (S**2 + 0.001))) @ (U.T @ y)\n",
    "\n",
    "sigma2 = squareSum(y - X @ theta)\n",
    "sigma = sigma2 ** 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "assignment destination is read-only",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-8034fa95e252>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0minv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXTX\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdiagr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minv\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mXTX\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0minv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiagonal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdiagr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdiagr\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: assignment destination is read-only"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "inv = np.linalg.inv(XTX + np.eye(p)*0.01)\n",
    "diagr = (inv @ XTX @ inv).diagonal()\n",
    "cond = diagr <= 0\n",
    "mins = diagr[cond].min()\n",
    "diagr[cond] = mins\n",
    "diagr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inv = linalg.eigh(X @ X.T, check_finite = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.074567124\n",
      "Wall time: 481 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "XT = X.T\n",
    "XTX = XT @ X\n",
    "from torch import potrf as cholesky, potrs as cholesky_triangular_solve\n",
    "chol = cholesky(  Tensor(XTX)  )\n",
    "t = cholesky_triangular_solve( Tensor(XT) @ Tensor(y), chol).flatten().numpy()\n",
    "print(squareSum(y - X @ t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinvv(X):\n",
    "    U, S, VT = linalg.svd(X, full_matrices = False, check_finite = False)\n",
    "    cond =  S < np.finfo(X.dtype).eps*S[0] \n",
    "    S = np.divide(1.0, S)\n",
    "    S[cond] = 0.0\n",
    "    VT *= S.reshape(-1,1)\n",
    "    return VT.T @ U.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t = np.linalg.eigh(XXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squareSum(y - X @ t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "XXT = X @ X.T\n",
    "# XTX = X.T @ X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import Ridge\n",
    "model = Ridge(fit_intercept = False, alpha = 1, solver = 'cholesky')\n",
    "model.fit(X, y)\n",
    "# print(squareSum(y - model.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg \n",
    "from scipy.linalg import lapack\n",
    "U = linalg.cholesky(XTX, lower = False, check_finite = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_U = X @ lapack.strtri(U)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_U * X_U).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_U = linalg.cho_solve((U, False), np.eye(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_U = lapack.dtrtri(U)[0]\n",
    "(_U * _U).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__\n",
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "j = torch.svd(Tensor(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "U, S, VT = torch.svd(Tensor(X))\n",
    "cond = S < eps(X)*constant(S[0])\n",
    "_S = 1.0 / S\n",
    "_S[cond] = 0.0\n",
    "VT *= _S.reshape(-1,1)\n",
    "UT = U.numpy().T\n",
    "inv = VT.numpy().T.dot(UT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inv = torch.pinverse(Tensor(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t = torch.gels(Tensor(y).reshape(-1,1), Tensor(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "XTX = X.T.dot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# x = Tensor(X)\n",
    "XTX = x.t().matmul(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "a = torch.mm(x.t(), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from scipy import linalg\n",
    "a = linalg.pinv2(XTX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "X.T @ X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = Tensor(XT)\n",
    "x @ Tensor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "XT = X.T\n",
    "XTX = XT.dot(X)\n",
    "# theta_hat = pinv(XTX).dot(XT.dot(y))\n",
    "# print(((y - X.dot(theta_hat))**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "a = x.numpy()\n",
    "a.T.dot(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.linalg as linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table('C:/Users/danie/Downloads/overdue.txt')\n",
    "data.columns\n",
    "data['isResid'] = 0\n",
    "data['isComm'] = 0\n",
    "data.loc[:48, 'isResid'] = 1\n",
    "data.loc[48:, 'isComm'] = 1\n",
    "data['Residential'] = data['BILL']*data['isResid']\n",
    "data['Commercial'] = data['BILL']*data['isComm']\n",
    "data['Bias'] = 1\n",
    "y = data.pop('LATE')\n",
    "data.pop('BILL');\n",
    "\n",
    "X = data.values\n",
    "thetas = linalg.pinv2(X.T.dot(X)).dot( X.T.dot(y)  )\n",
    "y_hat = X.dot(thetas)\n",
    "\n",
    "e_hat = y - y_hat\n",
    "e_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y, e_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(residential['BILL'], residential['LATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inv = pinv(XTX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "a = svd(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "theta_hat = svd_solve(X, y)\n",
    "print(((y - X.dot(theta_hat))**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cholesky_solve(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((y - X.dot(svd_solve(X, y)))**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg.lapack import clapack\n",
    "Tensor(clapack.dtrtri(R)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import trtrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_all_warnings = True\n",
    "\n",
    "from torch import potrf as cholesky_decomposition, diag, ones, \\\n",
    "                potrs as cholesky_triangular_solve\n",
    "from numpy import round as np_round\n",
    "\n",
    "def t_cholesky_solve(X, y, alpha = 0, step = 2):\n",
    "    '''\n",
    "    Solve least squares problem X*theta_hat = y using Cholesky Decomposition.\n",
    "    \n",
    "    Alpha = 0, Step = 2 can be options\n",
    "    Alpha is Regularization Term and step = 2 default for guaranteed stability.\n",
    "    Step must be > 1\n",
    "    \n",
    "    |  Method   |   Operations    | Factor * np^2 |\n",
    "    |-----------|-----------------|---------------|\n",
    "    | Cholesky  |   1/3 * np^2    |      1/3      |\n",
    "    |    QR     |   p^3/3 + np^2  |   1 - p/3n    |\n",
    "    |    SVD    |   p^3   + np^2  |    1 - p/n    |\n",
    "    \n",
    "    NOTE: HyperLearn's implementation of Cholesky Solve uses L2 Regularization to enforce stability.\n",
    "    Cholesky is known to fail on ill-conditioned problems, so adding L2 penalties helpes it.\n",
    "    \n",
    "    Note, the algorithm in this implementation is as follows:\n",
    "    \n",
    "        alpha = dtype(X).decimal    [1e-6 is float32]\n",
    "        while failure {\n",
    "            solve cholesky ( XTX + alpha*identity )\n",
    "            alpha *= step (2 default)\n",
    "        }\n",
    "    \n",
    "    If MSE (Mean Squared Error) is abnormally high, it might be better to solve using stabler but\n",
    "    slower methods like qr_solve, svd_solve or lstsq.\n",
    "    \n",
    "    https://www.quora.com/Is-it-better-to-do-QR-Cholesky-or-SVD-for-solving-least-squares-estimate-and-why\n",
    "    '''\n",
    "    assert step > 1\n",
    "    \n",
    "    XTX = T(X).matmul(X)\n",
    "    regularizer = ones(X.shape[1]).type(X.dtype)\n",
    "    \n",
    "    if alpha == 0: \n",
    "        alpha = typeTensor([np_finfo(dtype(X)).resolution]).type(X.dtype)\n",
    "    no_success = True\n",
    "    warn = False\n",
    "\n",
    "    while no_success:\n",
    "        alphaI = regularizer*alpha\n",
    "        try:\n",
    "            chol = cholesky_decomposition(  XTX + diag(alphaI)  )\n",
    "            no_success = False\n",
    "        except:\n",
    "            alpha *= step\n",
    "            warn = True\n",
    "            \n",
    "    if warn and print_all_warnings:\n",
    "        addon = np_round(constant(alpha), 10)\n",
    "        print(f'''\n",
    "            Matrix is not full rank. Added regularization = {addon} to combat this. \n",
    "            Now, solving L2 regularized (XTX+{addon}*I)^-1(XTy).\n",
    "\n",
    "            NOTE: It might be better to use svd_solve, qr_solve or lstsq if MSE is high.\n",
    "            ''')\n",
    "   \n",
    "    XTy = T(X).matmul( ravel(y, chol)  )\n",
    "    \n",
    "    theta_hat = cholesky_triangular_solve(XTy, chol).flatten()\n",
    "    return theta_hat\n",
    "\n",
    "cholesky_solve = n2n(t_cholesky_solve)\n",
    "_cholesky_solve = n2t(t_cholesky_solve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "theta_hat = cholesky_solve(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = X.dot(theta_hat)\n",
    "e_hat = y - preds\n",
    "MSE = (e_hat**2).sum() / (n-p)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = X.dot(theta_hat_svd)\n",
    "e_hat = y - preds\n",
    "MSE = (e_hat**2).sum() / (n-p)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.square(Yx - X.dot(t)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "t = _lstsq(X, Yx)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = toTensor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "XTX = XX.t().matmul(XX)\n",
    "XTXt = X.T.dot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = tf.convert_to_tensor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "XTX = tf.matmul(X, X, adjoint_a = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_solve(X, y.astype(np.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse = torch.potri(chol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "theta = torch.potrs(XTy, chol)\n",
    "XX.matmul(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cholt = tf.linalg.cholesky(XTXt)\n",
    "XTy = X.T.dot(Ys.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "theta = tf.linalg.cholesky_solve(cholt, XTy)\n",
    "X.dot(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.linalg.lstsq(X, Yx.reshape(-1,1).astype(np.float32),\n",
    "                    l2_regularizer = 1.0/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((Yx - X.dot(T).ravel())**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperlearn.temp import addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parallel(addition)([1,5,1], [2,2,2], [3,3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X == 0).dtype is np.bool_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from hyperlearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "model = QuadraticDiscriminantAnalysis(n_jobs = -1)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "j = X - model.means_[0]\n",
    "out = toTensor(j).matmul(model.scaled_rotations_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "model = QuadraticDiscriminantAnalysis()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "alpha = 0.001\n",
    "a_1 = 1-alpha\n",
    "\n",
    "class_priors = y.bincount().type(torch.float32) / X.shape[0]\n",
    "classes = y.unique()\n",
    "\n",
    "class_scalings = []; class_rotations = []; class_means = []; log_scalings = []\n",
    "\n",
    "for x in classes:\n",
    "    partial_X = X[y == x]\n",
    "    partial_mean = partial_X.mean(0)\n",
    "    partial_X -= partial_mean\n",
    "    \n",
    "    U, S, VT = _svd(partial_X)\n",
    "    V = T(VT)\n",
    "    scale = (S**2) / (partial_X.shape[0] -1)\n",
    "    scale = alpha + (a_1 * scale)\n",
    "    \n",
    "    #partial_cov = (V * scale).matmul(VT)\n",
    "    \n",
    "    class_scalings.append(scale)\n",
    "    log_scalings.append(scale.log().sum())\n",
    "    class_rotations.append(V)\n",
    "    class_means.append(partial_mean)\n",
    "    \n",
    "class_log_scalings = stack(log_scalings)\n",
    "class_log_priors = class_priors.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "distances = []\n",
    "\n",
    "for V, S, means in zip(class_rotations, class_scalings, class_means):\n",
    "    partial_X = (X - means).matmul(   V/S**0.5   )\n",
    "    \n",
    "    plt.scatter(partial_X[:,0], partial_X[:,1], c = y, cmap = 'magma', alpha = 0.3)\n",
    "    plt.show()\n",
    "    distances.append(  einsum('ij,ij->i', partial_X, partial_X) )\n",
    "    #distances.append(  (partial_X**2).sum(1)  )\n",
    "\n",
    "distances = T(stack(distances))\n",
    "decision = -0.5 * (distances + class_log_scalings) + class_log_priors\n",
    "\n",
    "likelihood = (decision - T(decision.max(1)[0])).exp()\n",
    "sum_softmax = T( einsum('ij->i', likelihood) )\n",
    "#sum_softmax = T(likelihood.sum(1))\n",
    "softmax = likelihood / sum_softmax\n",
    "\n",
    "pred = classes.take(softmax.argmax(1)).numpy()\n",
    "true = y.numpy()\n",
    "print((pred == true).sum() / len(X) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-0.5*(distances + class_log_scalings) + class_log_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_shape = (X.shape[1], ), activation = 'relu') )\n",
    "model.add(Dense(5, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['acc'])\n",
    "model.fit(X.numpy(), labels.type(torch.float).numpy(), epochs = 10, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(X.shape[1], 5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(100, 5)\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), weight_decay = 0.001, lr = 0.05)\n",
    "\n",
    "labels = []\n",
    "for i in y.unique():\n",
    "    labels.append(y==i)\n",
    "labels = T(stack(labels)).type(torch.LongTensor )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = y.type(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, ys)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print((model(X).argmax(1) == ys).sum().type(torch.float) / len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(n_jobs = -1, multi_class = 'ovr')\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "model = QuadraticDiscriminantAnalysis(reg_param = 0.01)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "400/2700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "(pred == y).sum().item() / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_scalings[0].numpy().round(1), model.scalings_[0].round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "size = len(X)\n",
    "x = torch.randperm(size)\n",
    "\n",
    "x_train, x_test = X[x].chunk(2)\n",
    "y_train, y_test = y[x].chunk(2)\n",
    "size = len(x_test)\n",
    "\n",
    "y_true = y_test.numpy()\n",
    "class_priors = y_train.bincount().type(torch.float32) / x_train.shape[0]\n",
    "class_log_priors = class_priors.log()\n",
    "\n",
    "classes = y_train.unique()\n",
    "\n",
    "scalings = []\n",
    "rotations = []\n",
    "means = []\n",
    "log_scalings = []\n",
    "\n",
    "for x in classes:\n",
    "    partial_X = x_train[y_train == x, :]\n",
    "    partial_mean = partial_X.mean(0)\n",
    "    partial_X -= partial_mean\n",
    "    \n",
    "    U, S, VT = _svd(partial_X)\n",
    "    V = T(VT)\n",
    "    scale = (S**2) / (partial_X.shape[0] -1)\n",
    "    \n",
    "    scalings.append(scale)\n",
    "    rotations.append(V)\n",
    "    means.append(partial_mean)\n",
    "\n",
    "    \n",
    "scores = []\n",
    "alphas = np.arange(0.001, 1, 0.1)\n",
    "\n",
    "partials = []\n",
    "for x in class_means:\n",
    "    partials.append( x_test - x )\n",
    "    \n",
    "for alpha in alphas:\n",
    "    log_scalings = []\n",
    "    distances = []\n",
    "    \n",
    "    a_1 = 1 - alpha\n",
    "    \n",
    "    for partial_X, V, S in zip(partials, rotations, scalings):\n",
    "        scale = alpha + (a_1 * S)\n",
    "        log_scalings.append(scale.log().sum())\n",
    "        \n",
    "        partial_X = partial_X.matmul(   V/(scale**0.5)   )\n",
    "        \n",
    "        #distances.append(   (partial_X**2).sum(1)   )\n",
    "        distances.append(   einsum('ij,ij->i', partial_X, partial_X)   )\n",
    "    \n",
    "    class_log_scalings = stack(log_scalings)\n",
    "    distances = T(stack(distances))\n",
    "    decision = -0.5 * (distances + class_log_scalings) + class_log_priors\n",
    "\n",
    "    likelihood = (decision - T(decision.max(1)[0])).exp()\n",
    "    \n",
    "    sum_softmax = T(likelihood.sum(1))\n",
    "    #sum_softmax = T(toTensor(einsum('ij->i', likelihood)))\n",
    "    softmax = likelihood / sum_softmax\n",
    "\n",
    "    pred = classes.take(softmax.argmax(1)).numpy()\n",
    "    scores.append((pred == y_true).sum()/size)\n",
    "scores = np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas[scores.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(alphas, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaed = model.transform(centered)\n",
    "plt.scatter(x = pcaed[:,0], y = pcaed[:,1], c = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if self.priors is None:\n",
    "            self.priors_ = np.bincount(y) / float(n_samples)\n",
    "        else:\n",
    "            self.priors_ = self.priors\n",
    "\n",
    "        cov = None\n",
    "        store_covariance = self.store_covariance or self.store_covariances\n",
    "        if self.store_covariances:\n",
    "            warnings.warn(\"'store_covariances' was renamed to store_covariance\"\n",
    "                          \" in version 0.19 and will be removed in 0.21.\",\n",
    "                          DeprecationWarning)\n",
    "        if store_covariance:\n",
    "            cov = []\n",
    "        means = []\n",
    "        scalings = []\n",
    "        rotations = []\n",
    "        for ind in xrange(n_classes):\n",
    "            Xg = X[y == ind, :]\n",
    "            meang = Xg.mean(0)\n",
    "            means.append(meang)\n",
    "            if len(Xg) == 1:\n",
    "                raise ValueError('y has only 1 sample in class %s, covariance '\n",
    "                                 'is ill defined.' % str(self.classes_[ind]))\n",
    "            Xgc = Xg - meang\n",
    "            # Xgc = U * S * V.T\n",
    "            U, S, Vt = np.linalg.svd(Xgc, full_matrices=False)\n",
    "            rank = np.sum(S > self.tol)\n",
    "            if rank < n_features:\n",
    "                warnings.warn(\"Variables are collinear\")\n",
    "            S2 = (S ** 2) / (len(Xg) - 1)\n",
    "            S2 = ((1 - self.reg_param) * S2) + self.reg_param\n",
    "            if self.store_covariance or store_covariance:\n",
    "                # cov = V * (S^2 / (n-1)) * V.T\n",
    "                cov.append(np.dot(S2 * Vt.T, Vt))\n",
    "            scalings.append(S2)\n",
    "            rotations.append(Vt.T)\n",
    "        if self.store_covariance or store_covariance:\n",
    "            self.covariance_ = cov\n",
    "        self.means_ = np.asarray(means)\n",
    "        self.scalings_ = scalings\n",
    "        self.rotations_ = rotations\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, VT = _svd(X)\n",
    "Q, R = _qr(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ridge_stats(U, S, VT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_stats(U, S, VT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "einsum('ij,ij->i', (U, U) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.einsum('ij,ij->i', UU, UU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UU, SS, VTVT = svd(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n3 -r1\n",
    "U, S, VT = svd(X)\n",
    "U = S = VT = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit  -n3 -r1\n",
    "Q, R = qr(X)\n",
    "Q = R = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vt =_svd(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "u.matmul(torch.diag(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    max_U = np_argmax(np_abs(U), axis = 0)\n",
    "    signs = np_sign(U[max_U, range(U.shape[1])])\n",
    "    U *= signs\n",
    "    VT *= signs[:, np_newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_pinv(Tensor(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    U, S, VT = np_svd(X, full_matrices = False)\n",
    "    cond =  S < np_finfo(X.dtype).eps*S[0] \n",
    "    S = np_divide(1.0, S)\n",
    "    S[cond] = 0.0\n",
    "    VT *= S.reshape(-1,1)\n",
    "    return np_dot(VT.T, U.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_pinv(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.diag(_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.random((100000,100))\n",
    "X = X.astype(dtype = np.float32)\n",
    "# X = Tensor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = Tensor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "svd(X, some = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "tf.matmul( tf.multiply(U, S) , V, transpose_b = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = nn.Tensor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "U, S, V = nn.svd(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.transpose(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n3 -r1\n",
    "U, S, VT = _svd(X)\n",
    "U = S = VT = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n3 -r1\n",
    "Q, R = _qr(X)\n",
    "Q = R = None\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
