

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>hyperlearn package &mdash; HyperLearn 1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="hyperlearn.base" href="../base.html" />
    <link rel="prev" title="hyperlearn" href="modules.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> HyperLearn
          

          
          </a>

          
            
            
              <div class="version">
                0.0.2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">hyperlearn</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">hyperlearn package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-hyperlearn.base">hyperlearn.base module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-hyperlearn.linalg">hyperlearn.linalg module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-hyperlearn.utils">hyperlearn.utils module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-hyperlearn.random">hyperlearn.random module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-hyperlearn.exceptions">hyperlearn.exceptions module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hyperlearn-multiprocessing-module">hyperlearn.multiprocessing module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-hyperlearn.numba">hyperlearn.numba module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-hyperlearn.solvers">hyperlearn.solvers module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-hyperlearn.stats">hyperlearn.stats module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-hyperlearn.big_data.base">hyperlearn.big_data.base module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-hyperlearn.big_data.incremental">hyperlearn.big_data.incremental module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-hyperlearn.big_data.lsmr">hyperlearn.big_data.lsmr module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-hyperlearn.big_data.randomized">hyperlearn.big_data.randomized module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-hyperlearn.big_data.truncated">hyperlearn.big_data.truncated module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-hyperlearn.decomposition.base">hyperlearn.decomposition.base module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hyperlearn-decomposition-nmf-module">hyperlearn.decomposition.NMF module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-hyperlearn.decomposition.PCA">hyperlearn.decomposition.PCA module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id9">hyperlearn.decomposition.PCA module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hyperlearn-discriminant-analysis-base-module">hyperlearn.discriminant_analysis.base module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hyperlearn-discriminant-analysis-qda-module">hyperlearn.discriminant_analysis.QDA module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-hyperlearn.impute.SVDImpute">hyperlearn.impute.SVDImpute module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-hyperlearn.metrics.cosine">hyperlearn.metrics.cosine module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-hyperlearn.metrics.euclidean">hyperlearn.metrics.euclidean module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-hyperlearn.metrics.pairwise">hyperlearn.metrics.pairwise module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-hyperlearn.sparse.base">hyperlearn.sparse.base module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-hyperlearn.sparse.csr">hyperlearn.sparse.csr module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-hyperlearn.sparse.tcsr">hyperlearn.sparse.tcsr module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-hyperlearn">Module contents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../base.html">hyperlearn.base</a></li>
<li class="toctree-l1"><a class="reference internal" href="../linalg.html">hyperlearn.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html#contact">Contact</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">HyperLearn</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="modules.html">hyperlearn</a> &raquo;</li>
        
      <li>hyperlearn package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/source/hyperlearn.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="hyperlearn-package">
<h1>hyperlearn package<a class="headerlink" href="#hyperlearn-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-hyperlearn.base">
<span id="hyperlearn-base-module"></span><h2>hyperlearn.base module<a class="headerlink" href="#module-hyperlearn.base" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="hyperlearn.base.Numpy">
<code class="descclassname">hyperlearn.base.</code><code class="descname">Numpy</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/base.html#Numpy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.base.Numpy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.base.T">
<code class="descclassname">hyperlearn.base.</code><code class="descname">T</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/base.html#T"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.base.T" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.base.Tensor">
<code class="descclassname">hyperlearn.base.</code><code class="descname">Tensor</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/base.html#Tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.base.Tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.base.Tensors">
<code class="descclassname">hyperlearn.base.</code><code class="descname">Tensors</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/base.html#Tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.base.Tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="data">
<dt id="hyperlearn.base.USE_NUMBA">
<code class="descclassname">hyperlearn.base.</code><code class="descname">USE_NUMBA</code><em class="property"> = True</em><a class="headerlink" href="#hyperlearn.base.USE_NUMBA" title="Permalink to this definition">¶</a></dt>
<dd><p>Type Checks
Updated 27/8/2018
————————————————————</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.base.array">
<code class="descclassname">hyperlearn.base.</code><code class="descname">array</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/base.html#array"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.base.array" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.base.cast">
<code class="descclassname">hyperlearn.base.</code><code class="descname">cast</code><span class="sig-paren">(</span><em>X</em>, <em>dtype</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/base.html#cast"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.base.cast" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.base.check">
<code class="descclassname">hyperlearn.base.</code><code class="descname">check</code><span class="sig-paren">(</span><em>f</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/base.html#check"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.base.check" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.base.constant">
<code class="descclassname">hyperlearn.base.</code><code class="descname">constant</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/base.html#constant"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.base.constant" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.base.diag">
<code class="descclassname">hyperlearn.base.</code><code class="descname">diag</code><span class="sig-paren">(</span><em>input</em>, <em>diagonal=0</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#hyperlearn.base.diag" title="Permalink to this definition">¶</a></dt>
<dd><ul class="simple">
<li>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a vector (1-D tensor), then returns a 2-D square tensor
with the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> as the diagonal.</li>
<li>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a matrix (2-D tensor), then returns a 1-D tensor with
the diagonal elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</li>
</ul>
<p>The argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">diagonal</span></code> controls which diagonal to consider:</p>
<ul class="simple">
<li>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">diagonal</span></code> = 0, it is the main diagonal.</li>
<li>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">diagonal</span></code> &gt; 0, it is above the main diagonal.</li>
<li>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">diagonal</span></code> &lt; 0, it is below the main diagonal.</li>
</ul>
<dl class="docutils">
<dt>Args:</dt>
<dd>input (Tensor): the input tensor
diagonal (int, optional): the diagonal to consider
out (Tensor, optional): the output tensor</dd>
</dl>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diagonal()</span></code> always returns the diagonal of its input.</p>
<p class="last"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diagflat()</span></code> always constructs a tensor with diagonal elements
specified by the input.</p>
</div>
<p>Examples:</p>
<p>Get the square matrix where the input vector is the diagonal:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.5950,-0.0872, 2.3298])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([[ 0.5950, 0.0000, 0.0000],</span>
<span class="go">        [ 0.0000,-0.0872, 0.0000],</span>
<span class="go">        [ 0.0000, 0.0000, 2.3298]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 0.0000, 0.5950, 0.0000, 0.0000],</span>
<span class="go">        [ 0.0000, 0.0000,-0.0872, 0.0000],</span>
<span class="go">        [ 0.0000, 0.0000, 0.0000, 2.3298],</span>
<span class="go">        [ 0.0000, 0.0000, 0.0000, 0.0000]])</span>
</pre></div>
</div>
<p>Get the k-th diagonal of a given matrix:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[-0.4264, 0.0255,-0.1064],</span>
<span class="go">        [ 0.8795,-0.2429, 0.1374],</span>
<span class="go">        [ 0.1029,-0.6482,-1.6300]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([-0.4264,-0.2429,-1.6300])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([ 0.0255, 0.1374])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.base.diagSum">
<code class="descclassname">hyperlearn.base.</code><code class="descname">diagSum</code><span class="sig-paren">(</span><em>X</em>, <em>Y</em>, <em>transpose_a=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/base.html#diagSum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.base.diagSum" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.base.dtype">
<code class="descclassname">hyperlearn.base.</code><code class="descname">dtype</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/base.html#dtype"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.base.dtype" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.base.einsum">
<code class="descclassname">hyperlearn.base.</code><code class="descname">einsum</code><span class="sig-paren">(</span><em>notation</em>, <em>*args</em>, <em>tensor=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/base.html#einsum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.base.einsum" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.base.eps">
<code class="descclassname">hyperlearn.base.</code><code class="descname">eps</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/base.html#eps"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.base.eps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.base.isArray">
<code class="descclassname">hyperlearn.base.</code><code class="descname">isArray</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/base.html#isArray"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.base.isArray" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.base.isDict">
<code class="descclassname">hyperlearn.base.</code><code class="descname">isDict</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/base.html#isDict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.base.isDict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.base.isIterable">
<code class="descclassname">hyperlearn.base.</code><code class="descname">isIterable</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/base.html#isIterable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.base.isIterable" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.base.isList">
<code class="descclassname">hyperlearn.base.</code><code class="descname">isList</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/base.html#isList"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.base.isList" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.base.isTensor">
<code class="descclassname">hyperlearn.base.</code><code class="descname">isTensor</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/base.html#isTensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.base.isTensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.base.ones">
<code class="descclassname">hyperlearn.base.</code><code class="descname">ones</code><span class="sig-paren">(</span><em>*sizes</em>, <em>out=None</em>, <em>dtype=None</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#hyperlearn.base.ones" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor filled with the scalar value <cite>1</cite>, with the shape defined
by the variable argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">sizes</span></code>.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first docutils">
<dt>sizes (int…): a sequence of integers defining the shape of the output tensor.</dt>
<dd>Can be a variable number of arguments or a collection like a list or tuple.</dd>
</dl>
<p>out (Tensor, optional): the output tensor
dtype (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional): the desired data type of returned tensor.</p>
<blockquote>
<div>Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code>).</div></blockquote>
<dl class="last docutils">
<dt>layout (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code>, optional): the desired layout of returned Tensor.</dt>
<dd>Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</dd>
<dt>device (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>, optional): the desired device of returned tensor.</dt>
<dd>Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</dd>
<dt>requires_grad (bool, optional): If autograd should record operations on the</dt>
<dd>returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</dd>
</dl>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  1.,  1.],</span>
<span class="go">        [ 1.,  1.,  1.]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="go">tensor([ 1.,  1.,  1.,  1.,  1.])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.base.resolution">
<code class="descclassname">hyperlearn.base.</code><code class="descname">resolution</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/base.html#resolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.base.resolution" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.base.return_numpy">
<code class="descclassname">hyperlearn.base.</code><code class="descname">return_numpy</code><span class="sig-paren">(</span><em>args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/base.html#return_numpy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.base.return_numpy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.base.return_torch">
<code class="descclassname">hyperlearn.base.</code><code class="descname">return_torch</code><span class="sig-paren">(</span><em>args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/base.html#return_torch"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.base.return_torch" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.base.rowSum">
<code class="descclassname">hyperlearn.base.</code><code class="descname">rowSum</code><span class="sig-paren">(</span><em>X</em>, <em>Y=None</em>, <em>transpose_a=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/base.html#rowSum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.base.rowSum" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.base.squareSum">
<code class="descclassname">hyperlearn.base.</code><code class="descname">squareSum</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/base.html#squareSum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.base.squareSum" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.base.stack">
<code class="descclassname">hyperlearn.base.</code><code class="descname">stack</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/base.html#stack"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.base.stack" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.base.t_einsum">
<code class="descclassname">hyperlearn.base.</code><code class="descname">t_einsum</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hyperlearn.base.t_einsum" title="Permalink to this definition">¶</a></dt>
<dd><p>einsum(equation, operands) -&gt; Tensor</p>
<p>This function provides a way of computing multilinear expressions (i.e. sums of products) using the
Einstein summation convention.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>equation (string): The equation is given in terms of lower case letters (indices) to be associated</dt>
<dd>with each dimension of the operands and result. The left hand side lists the operands
dimensions, separated by commas. There should be one index letter per tensor dimension.
The right hand side follows after <cite>-&gt;</cite> and gives the indices for the output.
If the <cite>-&gt;</cite> and right hand side are omitted, it implicitly defined as the alphabetically
sorted list of all indices appearing exactly once in the left hand side.
The indices not apprearing in the output are summed over after multiplying the operands
entries.
If an index appears several times for the same operand, a diagonal is taken.
Ellipses <cite>…</cite> represent a fixed number of dimensions. If the right hand side is inferred,
the ellipsis dimensions are at the beginning of the output.</dd>
<dt>operands (list of Tensors): The operands to compute the Einstein sum of.</dt>
<dd>Note that the operands are passed as a list, not as individual arguments.</dd>
</dl>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;i,j-&gt;ij&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>  <span class="c1"># outer product</span>
<span class="go">tensor([[-0.0570, -0.0286, -0.0231,  0.0197],</span>
<span class="go">        [ 1.2616,  0.6335,  0.5113, -0.4351],</span>
<span class="go">        [ 1.4452,  0.7257,  0.5857, -0.4984],</span>
<span class="go">        [-0.4647, -0.2333, -0.1883,  0.1603],</span>
<span class="go">        [-1.1130, -0.5588, -0.4510,  0.3838]])</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bn,anm,bm-&gt;ba&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">l</span><span class="p">,</span><span class="n">A</span><span class="p">,</span><span class="n">r</span><span class="p">))</span> <span class="c1"># compare torch.nn.functional.bilinear</span>
<span class="go">tensor([[-0.3430, -5.2405,  0.4494],</span>
<span class="go">        [ 0.3311,  5.5201, -3.0356]])</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">As</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Bs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bij,bjk-&gt;bik&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">As</span><span class="p">,</span> <span class="n">Bs</span><span class="p">))</span> <span class="c1"># batch matrix multiplication</span>
<span class="go">tensor([[[-1.0564, -1.5904,  3.2023,  3.1271],</span>
<span class="go">         [-1.6706, -0.8097, -0.8025, -2.1183]],</span>

<span class="go">        [[ 4.2239,  0.3107, -0.5756, -0.2354],</span>
<span class="go">         [-1.4558, -0.3460,  1.5087, -0.8530]],</span>

<span class="go">        [[ 2.8153,  1.8787, -4.3839, -1.2112],</span>
<span class="go">         [ 0.3728, -2.1131,  0.0921,  0.8305]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ii-&gt;i&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">A</span><span class="p">,))</span> <span class="c1"># diagonal</span>
<span class="go">tensor([-0.7825,  0.8291, -0.1936])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;...ii-&gt;...i&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">A</span><span class="p">,))</span> <span class="c1"># batch diagonal</span>
<span class="go">tensor([[-1.0864,  0.7292,  0.0569],</span>
<span class="go">        [-0.9725, -1.0270,  0.6493],</span>
<span class="go">        [ 0.5832, -1.1716, -1.5084],</span>
<span class="go">        [ 0.4041, -1.1690,  0.8570]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;...ij-&gt;...ji&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">A</span><span class="p">,))</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># batch permute</span>
<span class="go">torch.Size([2, 3, 5, 4])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.base.torch_dot">
<code class="descclassname">hyperlearn.base.</code><code class="descname">torch_dot</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hyperlearn.base.torch_dot" title="Permalink to this definition">¶</a></dt>
<dd><p>matmul(tensor1, tensor2, out=None) -&gt; Tensor</p>
<p>Matrix product of two tensors.</p>
<p>The behavior depends on the dimensionality of the tensors as follows:</p>
<ul class="simple">
<li>If both tensors are 1-dimensional, the dot product (scalar) is returned.</li>
<li>If both arguments are 2-dimensional, the matrix-matrix product is returned.</li>
<li>If the first argument is 1-dimensional and the second argument is 2-dimensional,
a 1 is prepended to its dimension for the purpose of the matrix multiply.
After the matrix multiply, the prepended dimension is removed.</li>
<li>If the first argument is 2-dimensional and the second argument is 1-dimensional,
the matrix-vector product is returned.</li>
<li>If both arguments are at least 1-dimensional and at least one argument is
N-dimensional (where N &gt; 2), then a batched matrix multiply is returned.  If the first
argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the
batched matrix multiply and removed after.  If the second argument is 1-dimensional, a
1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.
The non-matrix (i.e. batch) dimensions are <span class="xref std std-ref">broadcasted</span> (and thus
must be broadcastable).  For example, if <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor1</span></code> is a
<span class="math notranslate nohighlight">\((j \times 1 \times n \times m)\)</span> tensor and <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor2</span></code> is a <span class="math notranslate nohighlight">\((k \times m \times p)\)</span>
tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> will be an <span class="math notranslate nohighlight">\((j \times k \times n \times p)\)</span> tensor.</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The 1-dimensional dot product version of this function does not support an <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> parameter.</p>
</div>
<dl class="docutils">
<dt>Arguments:</dt>
<dd>tensor1 (Tensor): the first tensor to be multiplied
tensor2 (Tensor): the second tensor to be multiplied
out (Tensor, optional): the output tensor</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># vector x vector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># matrix x vector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># batched matrix x broadcasted vector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([10, 3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># batched matrix x batched matrix</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([10, 3, 5])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># batched matrix x broadcasted matrix</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([10, 3, 5])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="module-hyperlearn.linalg">
<span id="hyperlearn-linalg-module"></span><h2>hyperlearn.linalg module<a class="headerlink" href="#module-hyperlearn.linalg" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="hyperlearn.linalg.cholesky">
<code class="descclassname">hyperlearn.linalg.</code><code class="descname">cholesky</code><span class="sig-paren">(</span><em>XTX</em>, <em>alpha=None</em>, <em>fast=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/linalg.html#cholesky"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.linalg.cholesky" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the Cholesky Decompsition of a Hermitian Matrix
(Positive Symmetric Matrix) giving a Upper Triangular Matrix.</p>
<p>Cholesky Decomposition is used as the default solver in HyperLearn,
as it is super fast and allows regularization. HyperLearn’s
implementation also handles rank deficient and ill-conditioned
matrices perfectly with the help of the limiting behaivour of
adding forced epsilon regularization.</p>
<dl class="docutils">
<dt>If USE_GPU:</dt>
<dd>Uses PyTorch’s Cholesky. Speed is OK.</dd>
<dt>If CPU:</dt>
<dd>Uses Numpy’s Fortran C based Cholesky.
If NUMBA is not installed, uses very fast LAPACK functions.</dd>
</dl>
<p>Alpha is added for regularization purposes. This prevents system
rounding errors and promises better convergence rates.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.linalg.invCholesky">
<code class="descclassname">hyperlearn.linalg.</code><code class="descname">invCholesky</code><span class="sig-paren">(</span><em>X</em>, <em>fast=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/linalg.html#invCholesky"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.linalg.invCholesky" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the Inverse of a Hermitian Matrix
(Positive Symmetric Matrix) after provided with Cholesky’s
Lower Triangular Matrix.</p>
<p>This is used in conjunction in solveCholesky, where the
inverse of the covariance matrix is needed.</p>
<dl class="docutils">
<dt>If USE_GPU:</dt>
<dd>Uses PyTorch’s Triangular Solve given identity matrix. Speed is OK.</dd>
<dt>If CPU:</dt>
<dd>Uses very fast LAPACK algorithms for triangular system inverses.</dd>
</dl>
<p>Note that LAPACK’s single precision (float32) solver (strtri) is much more
unstable than double (float64). So, default strtri is OFF.
However, speeds are reduced by 50%.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.linalg.pinvCholesky">
<code class="descclassname">hyperlearn.linalg.</code><code class="descname">pinvCholesky</code><span class="sig-paren">(</span><em>X</em>, <em>alpha=None</em>, <em>fast=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/linalg.html#pinvCholesky"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.linalg.pinvCholesky" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the approximate pseudoinverse of any matrix using Cholesky Decomposition
This means X &#64; pinv(X) approx = eye(n).</p>
<p>Note that this is super fast, and will be used in HyperLearn as the default
pseudoinverse solver for any matrix. Care is taken to make the algorithm
converge, and this is done via forced epsilon regularization.</p>
<p>HyperLearn’s implementation also handles rank deficient and ill-conditioned
matrices perfectly with the help of the limiting behaivour of
adding forced epsilon regularization.</p>
<dl class="docutils">
<dt>If USE_GPU:</dt>
<dd>Uses PyTorch’s Cholesky. Speed is OK.</dd>
<dt>If CPU:</dt>
<dd>Uses Numpy’s Fortran C based Cholesky.
If NUMBA is not installed, uses very fast LAPACK functions.</dd>
</dl>
<p>Alpha is added for regularization purposes. This prevents system
rounding errors and promises better convergence rates.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.linalg.cholSolve">
<code class="descclassname">hyperlearn.linalg.</code><code class="descname">cholSolve</code><span class="sig-paren">(</span><em>A</em>, <em>b</em>, <em>alpha=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/linalg.html#cholSolve"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.linalg.cholSolve" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 20/10/2018]
Faster than direct inverse solve. Finds coefficients in linear regression
allowing A &#64; theta = b.
Notice auto adds epsilon jitter if solver fails.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.linalg.svd">
<code class="descclassname">hyperlearn.linalg.</code><code class="descname">svd</code><span class="sig-paren">(</span><em>X</em>, <em>fast=True</em>, <em>U_decision=False</em>, <em>transpose=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/linalg.html#svd"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.linalg.svd" title="Permalink to this definition">¶</a></dt>
<dd><p>[Edited 9/11/2018 –&gt; Modern Big Data Algorithms p/n ratio check]
Computes the Singular Value Decomposition of any matrix.
So, X = U * S &#64; VT. Note will compute svd(X.T) if p &gt; n.
Should be 99% same result. This means this implementation’s
time complexity is O[ min(np^2, n^2p) ]</p>
<dl class="docutils">
<dt>If USE_GPU:</dt>
<dd>Uses PyTorch’s SVD. PyTorch uses (for now) a NON divide-n-conquer algo.
Submitted report to PyTorch:
<a class="reference external" href="https://github.com/pytorch/pytorch/issues/11174">https://github.com/pytorch/pytorch/issues/11174</a></dd>
<dt>If CPU:</dt>
<dd>Uses Numpy’s Fortran C based SVD.
If NUMBA is not installed, uses divide-n-conqeur LAPACK functions.</dd>
<dt>If Transpose:</dt>
<dd>Will compute if possible svd(X.T) instead of svd(X) if p &gt; n.
Default setting is TRUE to maintain speed.</dd>
</dl>
<p>SVD_Flip is used for deterministic output. Does NOT follow Sklearn convention.
This flips the signs of U and VT, using VT_based decision.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.linalg.lu">
<code class="descclassname">hyperlearn.linalg.</code><code class="descname">lu</code><span class="sig-paren">(</span><em>X</em>, <em>L_only=False</em>, <em>U_only=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/linalg.html#lu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.linalg.lu" title="Permalink to this definition">¶</a></dt>
<dd><p>[Edited 8/11/2018 Changed to LAPACK LU if L/U only wanted]
Computes the LU Decomposition of any matrix with pivoting.
Provides L only or U only if specified.</p>
<p>Much faster than Scipy if only U/L wanted, and more memory efficient,
since data is altered inplace.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.linalg.qr">
<code class="descclassname">hyperlearn.linalg.</code><code class="descname">qr</code><span class="sig-paren">(</span><em>X</em>, <em>Q_only=False</em>, <em>R_only=False</em>, <em>overwrite=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/linalg.html#qr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.linalg.qr" title="Permalink to this definition">¶</a></dt>
<dd><p>[Edited 8/11/2018 Added Q, R only parameters. Faster than Numba]
[Edited 9/11/2018 Made R only more memory efficient (no data copying)]
Computes the reduced QR Decomposition of any matrix.
Uses optimized NUMBA QR if avaliable else use’s Scipy’s
version.</p>
<p>Provides Q or R only if specified, and is must faster + more memory
efficient since data is changed inplace.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.linalg.pinv">
<code class="descclassname">hyperlearn.linalg.</code><code class="descname">pinv</code><span class="sig-paren">(</span><em>X</em>, <em>alpha=None</em>, <em>fast=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/linalg.html#pinv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.linalg.pinv" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the pseudoinverse of any matrix.
This means X &#64; pinv(X) = eye(n).</p>
<p>Optional alpha is used for regularization purposes.</p>
<dl class="docutils">
<dt>If USE_GPU:</dt>
<dd>Uses PyTorch’s SVD. PyTorch uses (for now) a NON divide-n-conquer algo.
Submitted report to PyTorch:
<a class="reference external" href="https://github.com/pytorch/pytorch/issues/11174">https://github.com/pytorch/pytorch/issues/11174</a></dd>
<dt>If CPU:</dt>
<dd>Uses Numpy’s Fortran C based SVD.
If NUMBA is not installed, uses divide-n-conqeur LAPACK functions.</dd>
</dl>
<dl class="docutils">
<dt>Condition number is:</dt>
<dd>float32 = 1e3 * eps * max(S)
float64 = 1e6 * eps * max(S)</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.linalg.pinvh">
<code class="descclassname">hyperlearn.linalg.</code><code class="descname">pinvh</code><span class="sig-paren">(</span><em>XTX</em>, <em>alpha=None</em>, <em>fast=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/linalg.html#pinvh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.linalg.pinvh" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the pseudoinverse of a Hermitian Matrix
(Positive Symmetric Matrix) using Eigendecomposition.</p>
<p>Uses the fact that the matrix is special, and so time
complexity is approximately reduced by 1/2 or more when
compared to full SVD.</p>
<dl class="docutils">
<dt>If USE_GPU:</dt>
<dd>Uses PyTorch’s EIGH. PyTorch uses (for now) a non divide-n-conquer algo.</dd>
<dt>If CPU:</dt>
<dd>Uses Numpy’s Fortran C based EIGH.
If NUMBA is not installed, uses very fast divide-n-conqeur LAPACK functions.
Note Scipy’s EIGH as of now is NON divide-n-conquer.
Submitted report to Scipy:
<a class="reference external" href="https://github.com/scipy/scipy/issues/9212">https://github.com/scipy/scipy/issues/9212</a></dd>
</dl>
<dl class="docutils">
<dt>Condition number is:</dt>
<dd>float32 = 1e3 * eps * max(abs(S))
float64 = 1e6 * eps * max(abs(S))</dd>
</dl>
<p>Alpha is added for regularization purposes. This prevents system
rounding errors and promises better convergence rates.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.linalg.eigh">
<code class="descclassname">hyperlearn.linalg.</code><code class="descname">eigh</code><span class="sig-paren">(</span><em>XTX</em>, <em>alpha=None</em>, <em>fast=True</em>, <em>svd=False</em>, <em>positive=False</em>, <em>qr=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/linalg.html#eigh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.linalg.eigh" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the Eigendecomposition of a Hermitian Matrix
(Positive Symmetric Matrix).</p>
<p>Note: Slips eigenvalues / eigenvectors with MAX first.
Scipy convention is MIN first, but MAX first is SVD convention.</p>
<p>Uses the fact that the matrix is special, and so time
complexity is approximately reduced by 1/2 or more when
compared to full SVD.</p>
<p>If POSITIVE is True, then all negative eigenvalues will be set
to zero, and return value will be VT and not V.</p>
<p>If SVD is True, then eigenvalues will be square rooted as well.</p>
<dl class="docutils">
<dt>If USE_GPU:</dt>
<dd>Uses PyTorch’s EIGH. PyTorch uses (for now) a non divide-n-conquer algo.</dd>
<dt>If CPU:</dt>
<dd>Uses Numpy’s Fortran C based EIGH.
If NUMBA is not installed, uses very fast divide-n-conqeur LAPACK functions.
Note Scipy’s EIGH as of now is NON divide-n-conquer.
Submitted report to Scipy:
<a class="reference external" href="https://github.com/scipy/scipy/issues/9212">https://github.com/scipy/scipy/issues/9212</a></dd>
</dl>
<p>Alpha is added for regularization purposes. This prevents system
rounding errors and promises better convergence rates.</p>
<p>Also uses eig_flip to flip the signs of the eigenvectors
to ensure deterministic output.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.linalg.pinvEig">
<code class="descclassname">hyperlearn.linalg.</code><code class="descname">pinvEig</code><span class="sig-paren">(</span><em>X</em>, <em>alpha=None</em>, <em>fast=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/linalg.html#pinvEig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.linalg.pinvEig" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the approximate pseudoinverse of any matrix X
using Eigendecomposition on the covariance matrix XTX or XXT</p>
<dl class="docutils">
<dt>Uses a special trick where:</dt>
<dd>If n &gt;= p: X^-1 approx = (XT &#64; X)^-1 &#64; XT
If n &lt; p:  X^-1 approx = XT &#64; (X &#64; XT)^-1</dd>
</dl>
<dl class="docutils">
<dt>If USE_GPU:</dt>
<dd>Uses PyTorch’s EIGH. PyTorch uses (for now) a non divide-n-conquer algo.</dd>
<dt>If CPU:</dt>
<dd>Uses Numpy’s Fortran C based EIGH.
If NUMBA is not installed, uses very fast divide-n-conqeur LAPACK functions.
Note Scipy’s EIGH as of now is NON divide-n-conquer.
Submitted report to Scipy:
<a class="reference external" href="https://github.com/scipy/scipy/issues/9212">https://github.com/scipy/scipy/issues/9212</a></dd>
</dl>
<dl class="docutils">
<dt>Condition number is:</dt>
<dd>float32 = 1e3 * eps * max(abs(S))
float64 = 1e6 * eps * max(abs(S))</dd>
</dl>
<p>Alpha is added for regularization purposes. This prevents system
rounding errors and promises better convergence rates.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.linalg.eig">
<code class="descclassname">hyperlearn.linalg.</code><code class="descname">eig</code><span class="sig-paren">(</span><em>X</em>, <em>alpha=None</em>, <em>fast=True</em>, <em>U_decision=False</em>, <em>svd=False</em>, <em>stable=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/linalg.html#eig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.linalg.eig" title="Permalink to this definition">¶</a></dt>
<dd><p>[Edited 8/11/2018 Made QR-SVD even faster –&gt; changed to n &gt;= p from n &gt;= 5/3p]
Computes the Eigendecomposition of any matrix using either
QR then SVD or just SVD. This produces much more stable solutions
that pure eigh(covariance), and thus will be necessary in some cases.</p>
<p>If STABLE is True, then EIGH will be bypassed, and instead SVD or QR/SVD
will be used instead. This is to guarantee stability, since EIGH
uses epsilon jitter along the diagonal of the covariance matrix.</p>
<dl class="docutils">
<dt>If n &gt;= 5/3 * p:</dt>
<dd><p class="first">Uses QR followed by SVD noticing that U is not needed.
This means Q &#64; U is not required, reducing work.</p>
<p class="last">Note Sklearn’s Incremental PCA was used for the constant
5/3 [<cite>Matrix Computations, Third Edition, G. Holub and C.
Van Loan, Chapter 5, section 5.4.4, pp 252-253.</cite>]</p>
</dd>
<dt>Else If n &gt;= p:</dt>
<dd>SVD is used, as QR would be slower.</dd>
<dt>Else If n &lt;= p:</dt>
<dd>SVD Transpose is used svd(X.T)</dd>
<dt>If stable is False:</dt>
<dd>Eigh is used or SVD depending on the memory requirement.</dd>
</dl>
<p>Eig is the most stable Eigendecomposition in HyperLearn. It
surpasses the stability of Eigh, as no epsilon jitter is added,
unless specified when stable = False.</p>
</dd></dl>

</div>
<div class="section" id="module-hyperlearn.utils">
<span id="hyperlearn-utils-module"></span><h2>hyperlearn.utils module<a class="headerlink" href="#module-hyperlearn.utils" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="hyperlearn.utils.lapack">
<em class="property">class </em><code class="descclassname">hyperlearn.utils.</code><code class="descname">lapack</code><span class="sig-paren">(</span><em>function</em>, <em>fast=True</em>, <em>numba=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/utils.html#lapack"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.utils.lapack" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>[Added 11/11/2018] [Edited 13/11/2018 -&gt; made into a class]
[Edited 14/11/2018 -&gt; fixed class]
Get a LAPACK function based on the dtype(X). Acts like Scipy.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.utils.svd_flip">
<code class="descclassname">hyperlearn.utils.</code><code class="descname">svd_flip</code><span class="sig-paren">(</span><em>U</em>, <em>VT</em>, <em>U_decision=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/utils.html#svd_flip"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.utils.svd_flip" title="Permalink to this definition">¶</a></dt>
<dd><p>Flips the signs of U and VT for SVD in order to force deterministic output.</p>
<p>Follows Sklearn convention by looking at U’s maximum in columns
as default.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.utils.eig_flip">
<code class="descclassname">hyperlearn.utils.</code><code class="descname">eig_flip</code><span class="sig-paren">(</span><em>V</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/utils.html#eig_flip"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.utils.eig_flip" title="Permalink to this definition">¶</a></dt>
<dd><p>Flips the signs of V for Eigendecomposition in order to
force deterministic output.</p>
<p>Follows Sklearn convention by looking at V’s maximum in columns
as default. This essentially mirrors svd_flip(U_decision = False)</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.utils.memoryXTX">
<code class="descclassname">hyperlearn.utils.</code><code class="descname">memoryXTX</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/utils.html#memoryXTX"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.utils.memoryXTX" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the memory usage for X.T &#64; X so that error messages
can be broadcast without submitting to a memory error.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.utils.memoryCovariance">
<code class="descclassname">hyperlearn.utils.</code><code class="descname">memoryCovariance</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/utils.html#memoryCovariance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.utils.memoryCovariance" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the memory usage for X.T &#64; X or X &#64; X.T so that error messages
can be broadcast without submitting to a memory error.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.utils.memorySVD">
<code class="descclassname">hyperlearn.utils.</code><code class="descname">memorySVD</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/utils.html#memorySVD"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.utils.memorySVD" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the approximate memory usage of SVD(X) [transpose or not].
How it’s computed:</p>
<blockquote>
<div>X = U * S * VT
U(n,p) * S(p) * VT(p,p)
This means RAM usgae is np+p+p^2 approximately.</div></blockquote>
<p>### TODO: Divide N Conquer SVD vs old SVD</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.utils.traceXTX">
<code class="descclassname">hyperlearn.utils.</code><code class="descname">traceXTX</code><a class="reference internal" href="../_modules/hyperlearn/utils.html#traceXTX"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.utils.traceXTX" title="Permalink to this definition">¶</a></dt>
<dd><p>[Edited 18/10/2018]
One drawback of truncated algorithms is that they can’t output the correct
variance explained ratios, since the full eigenvalue decomp needs to be
done. However, using linear algebra, trace(XT*X) = sum(eigenvalues).</p>
<p>So, this function outputs the trace(XT*X) efficiently without computing
explicitly XT*X.</p>
<p>Changes –&gt; now uses Numba which is approx 20% faster.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.utils.fastDot">
<code class="descclassname">hyperlearn.utils.</code><code class="descname">fastDot</code><span class="sig-paren">(</span><em>A</em>, <em>B</em>, <em>C</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/utils.html#fastDot"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.utils.fastDot" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 23/9/2018]
[Updated 1/10/2018 Error in calculating which is faster]
Computes a fast matrix multiplication of 3 matrices.
Either performs (A &#64; B) &#64; C or A &#64; (B &#64; C) depending which is more
efficient.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.utils.rowSum">
<code class="descclassname">hyperlearn.utils.</code><code class="descname">rowSum</code><span class="sig-paren">(</span><em>X</em>, <em>norm=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/utils.html#rowSum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.utils.rowSum" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 22/10/2018]
Combines rowSum for matrices and arrays.</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.utils.rowSum_A">
<code class="descclassname">hyperlearn.utils.</code><code class="descname">rowSum_A</code><a class="reference internal" href="../_modules/hyperlearn/utils.html#rowSum_A"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.utils.rowSum_A" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 22/10/2018]
Computes rowSum**2 for dense array efficiently, instead of using einsum</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.utils.reflect">
<code class="descclassname">hyperlearn.utils.</code><code class="descname">reflect</code><span class="sig-paren">(</span><em>X</em>, <em>n_jobs=1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/utils.html#reflect"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.utils.reflect" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 15/10/2018] [Edited 18/10/2018]
Reflects lower triangular of matrix efficiently to upper.
Notice much faster than say X += X.T or naive:</p>
<blockquote>
<div><dl class="docutils">
<dt>for i in range(n):</dt>
<dd><dl class="first last docutils">
<dt>for j in range(i, n):</dt>
<dd>X[i,j] = X[j,i]</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<dl class="docutils">
<dt>In fact, it is much faster to perform vertically:</dt>
<dd><dl class="first last docutils">
<dt>for i in range(1, n):</dt>
<dd><p class="first">Xi = X[i]
for j in range(i):</p>
<blockquote class="last">
<div>X[j,i] = Xi[j]</div></blockquote>
</dd>
</dl>
</dd>
</dl>
<p>The trick is to notice X[i], which reduces array access.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.utils.addDiagonal">
<code class="descclassname">hyperlearn.utils.</code><code class="descname">addDiagonal</code><span class="sig-paren">(</span><em>X</em>, <em>c=1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/utils.html#addDiagonal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.utils.addDiagonal" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 11/11/2018]
Add c to diagonal of matrix</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.utils.setDiagonal">
<code class="descclassname">hyperlearn.utils.</code><code class="descname">setDiagonal</code><span class="sig-paren">(</span><em>X</em>, <em>c=1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/utils.html#setDiagonal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.utils.setDiagonal" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 11/11/2018]
Set c to diagonal of matrix</p>
</dd></dl>

</div>
<div class="section" id="module-hyperlearn.random">
<span id="hyperlearn-random-module"></span><h2>hyperlearn.random module<a class="headerlink" href="#module-hyperlearn.random" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="hyperlearn.random.uniform">
<code class="descclassname">hyperlearn.random.</code><code class="descname">uniform</code><span class="sig-paren">(</span><em>left</em>, <em>right</em>, <em>n</em>, <em>p=None</em>, <em>dtype=&lt;class 'numpy.float32'&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/random.html#uniform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.random.uniform" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 6/11/2018]</p>
<p>Produces pseudo-random uniform numbers between left and right range.
Notice much more memory efficient than Numpy, as provides
a DTYPE argument (float32 supported).</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.random.uniform_vector">
<code class="descclassname">hyperlearn.random.</code><code class="descname">uniform_vector</code><a class="reference internal" href="../_modules/hyperlearn/random.html#uniform_vector"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.random.uniform_vector" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-hyperlearn.exceptions">
<span id="hyperlearn-exceptions-module"></span><h2>hyperlearn.exceptions module<a class="headerlink" href="#module-hyperlearn.exceptions" title="Permalink to this headline">¶</a></h2>
<dl class="exception">
<dt id="hyperlearn.exceptions.FutureExceedsMemory">
<em class="property">exception </em><code class="descclassname">hyperlearn.exceptions.</code><code class="descname">FutureExceedsMemory</code><span class="sig-paren">(</span><em>text='Operation done in the future uses more memory than what is free. HyperLearn'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/exceptions.html#FutureExceedsMemory"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.exceptions.FutureExceedsMemory" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#BaseException" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseException</span></code></a></p>
</dd></dl>

<dl class="exception">
<dt id="hyperlearn.exceptions.PartialWrongShape">
<em class="property">exception </em><code class="descclassname">hyperlearn.exceptions.</code><code class="descname">PartialWrongShape</code><span class="sig-paren">(</span><em>text='Partial SVD or Eig needs the same number of columns in both the previous iteration and the future iteration. Currenlty</em>, <em>the number of columns is different.'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/exceptions.html#PartialWrongShape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.exceptions.PartialWrongShape" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#BaseException" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseException</span></code></a></p>
</dd></dl>

</div>
<div class="section" id="hyperlearn-multiprocessing-module">
<h2>hyperlearn.multiprocessing module<a class="headerlink" href="#hyperlearn-multiprocessing-module" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-hyperlearn.numba">
<span id="hyperlearn-numba-module"></span><h2>hyperlearn.numba module<a class="headerlink" href="#module-hyperlearn.numba" title="Permalink to this headline">¶</a></h2>
<dl class="attribute">
<dt id="hyperlearn.numba.svd">
<code class="descclassname">hyperlearn.numba.</code><code class="descname">svd</code><a class="reference internal" href="../_modules/hyperlearn/numba.html#svd"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.numba.svd" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.numba.pinv">
<code class="descclassname">hyperlearn.numba.</code><code class="descname">pinv</code><a class="reference internal" href="../_modules/hyperlearn/numba.html#pinv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.numba.pinv" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.numba.eigh">
<code class="descclassname">hyperlearn.numba.</code><code class="descname">eigh</code><a class="reference internal" href="../_modules/hyperlearn/numba.html#eigh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.numba.eigh" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.numba.cholesky">
<code class="descclassname">hyperlearn.numba.</code><code class="descname">cholesky</code><a class="reference internal" href="../_modules/hyperlearn/numba.html#cholesky"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.numba.cholesky" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.numba.lstsq">
<code class="descclassname">hyperlearn.numba.</code><code class="descname">lstsq</code><a class="reference internal" href="../_modules/hyperlearn/numba.html#lstsq"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.numba.lstsq" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.numba.qr">
<code class="descclassname">hyperlearn.numba.</code><code class="descname">qr</code><a class="reference internal" href="../_modules/hyperlearn/numba.html#qr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.numba.qr" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.numba.norm">
<code class="descclassname">hyperlearn.numba.</code><code class="descname">norm</code><a class="reference internal" href="../_modules/hyperlearn/numba.html#norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.numba.norm" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.numba.mean">
<code class="descclassname">hyperlearn.numba.</code><code class="descname">mean</code><span class="sig-paren">(</span><em>X</em>, <em>axis=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/numba.html#mean"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.numba.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.numba.sign">
<code class="descclassname">hyperlearn.numba.</code><code class="descname">sign</code><a class="reference internal" href="../_modules/hyperlearn/numba.html#sign"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.numba.sign" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.numba.arange">
<code class="descclassname">hyperlearn.numba.</code><code class="descname">arange</code><a class="reference internal" href="../_modules/hyperlearn/numba.html#arange"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.numba.arange" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.numba.minimum">
<code class="descclassname">hyperlearn.numba.</code><code class="descname">minimum</code><a class="reference internal" href="../_modules/hyperlearn/numba.html#minimum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.numba.minimum" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.numba.maximum">
<code class="descclassname">hyperlearn.numba.</code><code class="descname">maximum</code><a class="reference internal" href="../_modules/hyperlearn/numba.html#maximum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.numba.maximum" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.numba.multsum">
<code class="descclassname">hyperlearn.numba.</code><code class="descname">multsum</code><a class="reference internal" href="../_modules/hyperlearn/numba.html#multsum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.numba.multsum" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.numba.squaresum">
<code class="descclassname">hyperlearn.numba.</code><code class="descname">squaresum</code><a class="reference internal" href="../_modules/hyperlearn/numba.html#squaresum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.numba.squaresum" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-hyperlearn.solvers">
<span id="hyperlearn-solvers-module"></span><h2>hyperlearn.solvers module<a class="headerlink" href="#module-hyperlearn.solvers" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="hyperlearn.solvers.solve">
<code class="descclassname">hyperlearn.solvers.</code><code class="descname">solve</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>tol=1e-06</em>, <em>condition_limit=100000000.0</em>, <em>alpha=None</em>, <em>weights=None</em>, <em>copy=False</em>, <em>non_negative=False</em>, <em>max_iter=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/solvers.html#solve"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.solvers.solve" title="Permalink to this definition">¶</a></dt>
<dd><p>[As of 12/9/2018, an optional non_negative argument is added. Note as accurate as Scipy’s NNLS,
by copies ideas from gradient descent.]</p>
<p>[NOTE: as of 12/9/2018, LSMR is default in HyperLearn, replacing the 2nd fastest Cholesky
Solve. LSMR is 2-4 times faster, and uses N less memory]</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">WEIGHTS</span> <span class="ow">is</span> <span class="n">an</span> <span class="n">array</span> <span class="n">of</span> <span class="n">Weights</span> <span class="k">for</span> <span class="n">Weighted</span> <span class="o">/</span> <span class="n">Generalized</span> <span class="n">Least</span> <span class="n">Squares</span> <span class="p">[</span><span class="n">default</span> <span class="kc">None</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">theta</span> <span class="o">=</span> <span class="p">(</span><span class="n">XT</span><span class="o">*</span><span class="n">W</span><span class="o">*</span><span class="n">X</span><span class="p">)</span><span class="o">^-</span><span class="mi">1</span><span class="o">*</span><span class="p">(</span><span class="n">XT</span><span class="o">*</span><span class="n">W</span><span class="o">*</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>Implements extremely fast least squares LSMR using orthogonalization as seen in Scipy’s LSMR and
<a class="reference external" href="https://arxiv.org/abs/1006.0758">https://arxiv.org/abs/1006.0758</a> [LSMR: An iterative algorithm for sparse least-squares problems]
by David Fong, Michael Saunders.</p>
<p>Scipy’s version of LSMR is surprisingly slow, as some slow design factors were used
(ie np.sqrt(1 number) is slower than number**0.5, or min(a,b) is slower than using 1 if statement.)</p>
<p>ALPHA is provided for regularization purposes like Ridge Regression.</p>
<dl class="docutils">
<dt>This algorithm works well for Sparse Matrices as well, and the time complexity analysis is approx:</dt>
<dd><p class="first">X.T &#64; y   * min(n,p) times + 3 or so O(n) operations
==&gt; O(np)*min(n,p)
==&gt; either min(O(n^2p + n), O(np^2 + n))
<a href="#id1"><span class="problematic" id="id2">**</span></a>* Note if Weights is present, complexity increases.</p>
<blockquote class="last">
<div>Instead of fitting X^T*W*X, fits X^T/sqrt(W)
So, O(np+n) is needed extra.</div></blockquote>
</dd>
</dl>
<p>This complexity is much better than Cholesky Solve which is the next fastest in HyperLearn.
Cholesky requires O(np^2) for XT * X, then Cholesky needs an extra 1/3*O(np^2), then inversion
takes another 1/3*(np^2), and finally (XT*y) needs O(np).</p>
<p>So Cholesky needs O(5/3np^2 + np) &gt;&gt; min(O(n^2p + n), O(np^2 + n))</p>
<p>So by factor analysis, expect LSMR to be approx 2 times faster or so.</p>
<p>Interestingly, the Space Complexity is even more staggering. LSMR takes only maximum O(np^2) space
for the computation of XT * y + some overhead.</p>
<blockquote>
<div><dl class="docutils">
<dt><a href="#id3"><span class="problematic" id="id4">**</span></a>* Note if Weights is present, and COPY IS TRUE, then memory is DOUBLED.</dt>
<dd>Hence, try setting COPY to FALSE, memory will not change, and X will return back to its
original state afterwards.</dd>
</dl>
</div></blockquote>
<p>Cholesky requires XT * X space, which is already max O(n^2p) [which is huge].
Essentially, Cholesky shines when P is large, but N is small. LSMR is good for large N, medium P</p>
<p>Theta_hat = (XT * W * X)^-1 * (XT * y)
In other words in gradient descent / iterative solves solve:</p>
<blockquote>
<div><p>X * sqrt(W) * theta_hat = y * sqrt(W)</p>
<p>or: X*sqrt(W)  ==&gt;  y*sqrt(W)</p>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.solvers.solveCholesky">
<code class="descclassname">hyperlearn.solvers.</code><code class="descname">solveCholesky</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>alpha=None</em>, <em>fast=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/solvers.html#solveCholesky"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.solvers.solveCholesky" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt>[Added 23/9/2018 added matrix multiplication decisions (faster multiply)</dt>
<dd>ie: if (XTX)^1(XTy) or ((XTX)^-1XT)y is faster]</dd>
</dl>
<p>[Edited 20/10/2018 Major update - added LAPACK cholSolve –&gt; 20% faster]
[Edited 30/10/2018 Reduced RAM usage by clearing unused variables]</p>
<p>Computes the Least Squares solution to X &#64; theta = y using Cholesky
Decomposition. This is the default solver in HyperLearn.</p>
<p>Cholesky Solving is used as the 2nd default solver [as of 12/9/2018, default
has been switched to LSMR (called solve)] in HyperLearn,
as it is super fast and allows regularization. HyperLearn’s
implementation also handles rank deficient and ill-conditioned
matrices perfectly with the help of the limiting behaivour of
adding forced epsilon regularization.</p>
<p>Optional alpha is used for regularization purposes.</p>
<div class="line-block">
<div class="line">Method   |   Operations    | Factor * np^2 |</div>
</div>
<p><a href="#id10"><span class="problematic" id="id11">|-----------|</span></a>—————–<a href="#id12"><span class="problematic" id="id13">|---------------|</span></a>
| Cholesky  |   1/3 * np^2    |      1/3      |
|    QR     |   p^3/3 + np^2  |   1 - p/3n    |
|    SVD    |   p^3   + np^2  |    1 - p/n    |</p>
<dl class="docutils">
<dt>If USE_GPU:</dt>
<dd>Uses PyTorch’s Cholesky and Triangular Solve given identity matrix.
Speed is OK.</dd>
<dt>If CPU:</dt>
<dd>Uses Numpy’s Fortran C based Cholesky.
If NUMBA is not installed, uses very fast LAPACK functions.
Also, uses very fast LAPACK algorithms for triangular system inverses.</dd>
</dl>
<p>Note that LAPACK’s single precision (float32) solver (strtri) is much more
unstable than double (float64). You might see stability problems if FAST = TRUE.
Set it to FALSE if theres issues.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.solvers.solveSVD">
<code class="descclassname">hyperlearn.solvers.</code><code class="descname">solveSVD</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>n_components=None</em>, <em>alpha=None</em>, <em>fast=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/solvers.html#solveSVD"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.solvers.solveSVD" title="Permalink to this definition">¶</a></dt>
<dd><p>[Edited 6/11/2018 Added n_components for Partial Solving]
Computes the Least Squares solution to X &#64; theta = y using SVD.
Slow, but most accurate. Specify n_components to reduce overfitting.
Heurestic is 95% of variance is captured, if set to ‘auto’.</p>
<p>Optional alpha is used for regularization purposes.</p>
<dl class="docutils">
<dt>If USE_GPU:</dt>
<dd>Uses PyTorch’s SVD. PyTorch uses (for now) a NON divide-n-conquer algo.
Submitted report to PyTorch:
<a class="reference external" href="https://github.com/pytorch/pytorch/issues/11174">https://github.com/pytorch/pytorch/issues/11174</a></dd>
<dt>If CPU:</dt>
<dd>Uses Numpy’s Fortran C based SVD.
If NUMBA is not installed, uses divide-n-conqeur LAPACK functions.</dd>
</dl>
<dl class="docutils">
<dt>Condition number is:</dt>
<dd>float32 = 1e3 * eps * max(S)
float64 = 1e6 * eps * max(S)</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.solvers.solveEig">
<code class="descclassname">hyperlearn.solvers.</code><code class="descname">solveEig</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>alpha=None</em>, <em>fast=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/solvers.html#solveEig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.solvers.solveEig" title="Permalink to this definition">¶</a></dt>
<dd><p>[Edited 30/10/2018 Reduced RAM usage by clearing unused variables]</p>
<p>Computes the Least Squares solution to X &#64; theta = y using
Eigendecomposition on the covariance matrix XTX or XXT.
Medium speed and accurate, where this lies between
SVD and Cholesky.</p>
<p>Optional alpha is used for regularization purposes.</p>
<dl class="docutils">
<dt>If USE_GPU:</dt>
<dd>Uses PyTorch’s EIGH. PyTorch uses (for now) a non divide-n-conquer algo.
Submitted report to PyTorch:
<a class="reference external" href="https://github.com/pytorch/pytorch/issues/11174">https://github.com/pytorch/pytorch/issues/11174</a></dd>
<dt>If CPU:</dt>
<dd>Uses Numpy’s Fortran C based EIGH.
If NUMBA is not installed, uses divide-n-conqeur LAPACK functions.
Note Scipy’s EIGH as of now is NON divide-n-conquer.
Submitted report to Scipy:
<a class="reference external" href="https://github.com/scipy/scipy/issues/9212">https://github.com/scipy/scipy/issues/9212</a></dd>
</dl>
<dl class="docutils">
<dt>Condition number is:</dt>
<dd>float32 = 1e3 * eps * max(abs(S))
float64 = 1e6 * eps * max(abs(S))</dd>
</dl>
<p>Alpha is added for regularization purposes. This prevents system
rounding errors and promises better convergence rates.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.solvers.solvePartial">
<code class="descclassname">hyperlearn.solvers.</code><code class="descname">solvePartial</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>n_components=None</em>, <em>alpha=None</em>, <em>fast=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/solvers.html#solvePartial"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.solvers.solvePartial" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 6/11/2018]
Computes the Least Squares solution to X &#64; theta = y using Randomized SVD.
Much faster than normal SVD solving, and is not prone is overfitting.</p>
<p>Optional alpha is used for regularization purposes.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.solvers.lstsq">
<code class="descclassname">hyperlearn.solvers.</code><code class="descname">lstsq</code><span class="sig-paren">(</span><em>X</em>, <em>y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/solvers.html#lstsq"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.solvers.lstsq" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns normal Least Squares solution using LAPACK and Numba if
installed. PyTorch will default to Cholesky Solve.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.solvers.solveTLS">
<code class="descclassname">hyperlearn.solvers.</code><code class="descname">solveTLS</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>solver='truncated'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/solvers.html#solveTLS"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.solvers.solveTLS" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 6/11/2018]
Performs Total Least Squares based on the implementation in Wikipedia:
<a class="reference external" href="https://en.wikipedia.org/wiki/Total_least_squares">https://en.wikipedia.org/wiki/Total_least_squares</a>.
The naming is rather deceptive, as it doesn’t mean it’ll yield better
results than pure SVD solving.
Normal linear regression assumes Y|X has gaussian noise. TLS assumes this
AND X|Y has noise.</p>
<p>Two solvers - full, truncated. Truncated is much much faster, as smallest
eigen component is needed. Full solver uses Eigendecomposition, which is
much much slower, but more accurate.</p>
</dd></dl>

</div>
<div class="section" id="module-hyperlearn.stats">
<span id="hyperlearn-stats-module"></span><h2>hyperlearn.stats module<a class="headerlink" href="#module-hyperlearn.stats" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="hyperlearn.stats.corr">
<code class="descclassname">hyperlearn.stats.</code><code class="descname">corr</code><span class="sig-paren">(</span><em>X</em>, <em>y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/stats.html#corr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.stats.corr" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.stats.qr_stats">
<code class="descclassname">hyperlearn.stats.</code><code class="descname">qr_stats</code><span class="sig-paren">(</span><em>Q</em>, <em>R</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/stats.html#qr_stats"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.stats.qr_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>XTX^-1  =  RT * R</p>
<p>h = diag  Q * QT</p>
<p>mean(h) used for normalized leverage</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.stats.svd_stats">
<code class="descclassname">hyperlearn.stats.</code><code class="descname">svd_stats</code><span class="sig-paren">(</span><em>U</em>, <em>S</em>, <em>VT</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/stats.html#svd_stats"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.stats.svd_stats" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div>1</div></blockquote>
<dl class="docutils">
<dt>XTX^-1 =  V —– VT</dt>
<dd>S^2</dd>
</dl>
<p>h = diag U * UT</p>
<p>mean(h) used for normalized leverage</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.stats.ridge_stats">
<code class="descclassname">hyperlearn.stats.</code><code class="descname">ridge_stats</code><span class="sig-paren">(</span><em>U</em>, <em>S</em>, <em>VT</em>, <em>alpha=1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/stats.html#ridge_stats"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.stats.ridge_stats" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div>S^2</div></blockquote>
<dl class="docutils">
<dt>exp_theta_hat =  diag V ——— VT</dt>
<dd><p class="first">S^2 + aI</p>
<blockquote class="last">
<div>S^2</div></blockquote>
</dd>
<dt>var_theta_hat =  diag V ————- VT</dt>
<dd><blockquote class="first">
<div>(S^2 + aI)^2</div></blockquote>
<p class="last">1</p>
</dd>
<dt>XTX^-1 =  V ——— VT</dt>
<dd><p class="first">S^2 + aI</p>
<blockquote class="last">
<div>S^2</div></blockquote>
</dd>
<dt>h = diag U ——— UT</dt>
<dd>S^2 + aI</dd>
</dl>
<p>mean(h) used for normalized leverage</p>
</dd></dl>

</div>
<div class="section" id="module-hyperlearn.big_data.base">
<span id="hyperlearn-big-data-base-module"></span><h2>hyperlearn.big_data.base module<a class="headerlink" href="#module-hyperlearn.big_data.base" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-hyperlearn.big_data.incremental">
<span id="hyperlearn-big-data-incremental-module"></span><h2>hyperlearn.big_data.incremental module<a class="headerlink" href="#module-hyperlearn.big_data.incremental" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="hyperlearn.big_data.incremental.partialEig">
<code class="descclassname">hyperlearn.big_data.incremental.</code><code class="descname">partialEig</code><span class="sig-paren">(</span><em>batch</em>, <em>S2</em>, <em>V</em>, <em>ratio=1</em>, <em>solver='full'</em>, <em>tol=None</em>, <em>max_iter='auto'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/big_data/incremental.html#partialEig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.big_data.incremental.partialEig" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits a partial Eigendecomp after given old eigenvalues S2
and old eigenvector components V.</p>
<p>Note that V will be used as the number of old components,
so when calling truncated or randomized, will output a
specific number of eigenvectors and eigenvalues.</p>
<p>Checks if new batch’s size matches that of the old V.</p>
<dl class="docutils">
<dt>Note that PartialEig has different solvers. Either choose:</dt>
<dd><ol class="first last arabic simple">
<li><dl class="first docutils">
<dt>full</dt>
<dd>Solves full Eigendecompsition on the data. This is the most
stable and will guarantee the most robust results.
You can select the number of components to keep
within the model later.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>truncated</dt>
<dd>This keeps the top K right eigenvectors and top
k eigenvalues, as determined by n_components. Note full Eig
is not called for the truncated case, but rather ARPACK is called.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>randomized</dt>
<dd>Same as truncated, but instead of using ARPACK, uses
randomized Eig.</dd>
</dl>
</li>
</ol>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.big_data.incremental.partialSVD">
<code class="descclassname">hyperlearn.big_data.incremental.</code><code class="descname">partialSVD</code><span class="sig-paren">(</span><em>batch</em>, <em>S</em>, <em>VT</em>, <em>ratio=1</em>, <em>solver='full'</em>, <em>tol=None</em>, <em>max_iter='auto'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/big_data/incremental.html#partialSVD"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.big_data.incremental.partialSVD" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits a partial SVD after given old singular values S
and old components VT.</p>
<p>Note that VT will be used as the number of old components,
so when calling truncated or randomized, will output a
specific number of eigenvectors and singular values.</p>
<p>Checks if new batch’s size matches that of the old VT.</p>
<dl class="docutils">
<dt>Note that PartialSVD has different solvers. Either choose:</dt>
<dd><ol class="first last arabic simple">
<li><dl class="first docutils">
<dt>full</dt>
<dd>Solves full SVD on the data. This is the most
stable and will guarantee the most robust results.
You can select the number of components to keep
within the model later.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>truncated</dt>
<dd>This keeps the top K right eigenvectors and top
k right singular values, as determined by
n_components. Note full SVD is not called for the
truncated case, but rather ARPACK is called.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>randomized</dt>
<dd>Same as truncated, but instead of using ARPACK, uses
randomized SVD.</dd>
</dl>
</li>
</ol>
</dd>
</dl>
<p>Notice how Batch = U &#64; S &#64; VT. However, partialSVD returns
S, VT, and not U. In order to get U, you might consider using
the relation that X = U &#64; S &#64; VT, and approximating U by:</p>
<blockquote>
<div><p>X = U &#64; S &#64; VT
X &#64; V = U &#64; S
(X &#64; V)/S = U</p>
<p>So, U = (X &#64; V)/S, so you can output U from (X &#64; V)/S</p>
<p>You can also get U partially and slowly using reverseU.</p>
</div></blockquote>
</dd></dl>

</div>
<div class="section" id="module-hyperlearn.big_data.lsmr">
<span id="hyperlearn-big-data-lsmr-module"></span><h2>hyperlearn.big_data.lsmr module<a class="headerlink" href="#module-hyperlearn.big_data.lsmr" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="hyperlearn.big_data.lsmr.Orthogonalize">
<code class="descclassname">hyperlearn.big_data.lsmr.</code><code class="descname">Orthogonalize</code><span class="sig-paren">(</span><em>a</em>, <em>b</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/big_data/lsmr.html#Orthogonalize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.big_data.lsmr.Orthogonalize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.big_data.lsmr.floatType">
<code class="descclassname">hyperlearn.big_data.lsmr.</code><code class="descname">floatType</code><span class="sig-paren">(</span><em>dtype</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/big_data/lsmr.html#floatType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.big_data.lsmr.floatType" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="hyperlearn.big_data.lsmr.lsmr">
<code class="descclassname">hyperlearn.big_data.lsmr.</code><code class="descname">lsmr</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>tol=1e-06</em>, <em>condition_limit=100000000.0</em>, <em>alpha=0</em>, <em>threshold=1000000000000.0</em>, <em>non_negative=False</em>, <em>max_iter=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/big_data/lsmr.html#lsmr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.big_data.lsmr.lsmr" title="Permalink to this definition">¶</a></dt>
<dd><p>[As of 12/9/2018, an optional non_negative argument is added. Note as accurate as Scipy’s NNLS,
by copies ideas from gradient descent.]</p>
<p>Implements extremely fast least squares LSMR using orthogonalization as seen in Scipy’s LSMR and
<a class="reference external" href="https://arxiv.org/abs/1006.0758">https://arxiv.org/abs/1006.0758</a> [LSMR: An iterative algorithm for sparse least-squares problems]
by David Fong, Michael Saunders.</p>
<p>Scipy’s version of LSMR is surprisingly slow, as some slow design factors were used
(ie np.sqrt(1 number) is slower than number**0.5, or min(a,b) is slower than using 1 if statement.)</p>
<p>ALPHA is provided for regularization purposes like Ridge Regression.</p>
<dl class="docutils">
<dt>This algorithm works well for Sparse Matrices as well, and the time complexity analysis is approx:</dt>
<dd>X.T &#64; y   * min(n,p) times + 3 or so O(n) operations
==&gt; O(np)*min(n,p)
==&gt; either min(O(n^2p + n), O(np^2 + n))</dd>
</dl>
<p>This complexity is much better than Cholesky Solve which is the next fastest in HyperLearn.
Cholesky requires O(np^2) for XT * X, then Cholesky needs an extra 1/3*O(np^2), then inversion
takes another 1/3*(np^2), and finally (XT*y) needs O(np).</p>
<p>So Cholesky needs O(5/3np^2 + np) &gt;&gt; min(O(n^2p + n), O(np^2 + n))</p>
<p>So by factor analysis, expect LSMR to be approx 2 times faster or so.
Interestingly, the Space Complexity is even more staggering. LSMR takes only maximum O(np^2) space
for the computation of XT * y + some overhead.</p>
<p>Cholesky requires XT * X space, which is already max O(n^2p) [which is huge].
Essentially, Cholesky shines when P is large, but N is small. LSMR is good for large N, medium P</p>
</dd></dl>

</div>
<div class="section" id="module-hyperlearn.big_data.randomized">
<span id="hyperlearn-big-data-randomized-module"></span><h2>hyperlearn.big_data.randomized module<a class="headerlink" href="#module-hyperlearn.big_data.randomized" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="hyperlearn.big_data.randomized.randomizedEig">
<code class="descclassname">hyperlearn.big_data.randomized.</code><code class="descname">randomizedEig</code><span class="sig-paren">(</span><em>X</em>, <em>n_components=2</em>, <em>max_iter='auto'</em>, <em>solver='lu'</em>, <em>n_oversamples=10</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/big_data/randomized.html#randomizedEig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.big_data.randomized.randomizedEig" title="Permalink to this definition">¶</a></dt>
<dd><p>[Edited 9/11/2018 Fixed Eig_Flip]
HyperLearn’s Randomized Eigendecomposition is an extension of Sklearn’s
randomized SVD. HyperLearn notices that the computation of U is not necessary,
hence will use QR followed by SVD or just SVD depending on the situation.</p>
<p>Likewise, solver = LU is default, and follows randomizedSVD</p>
<p class="rubric">References</p>
<ul class="simple">
<li>Sklearn’s RandomizedSVD</li>
<li>Finding structure with randomness: Stochastic algorithms for constructing
approximate matrix decompositions
Halko, et al., 2009 <a class="reference external" href="http://arxiv.org/abs/arXiv:0909.4061">http://arxiv.org/abs/arXiv:0909.4061</a></li>
<li>A randomized algorithm for the decomposition of matrices
Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert</li>
<li>An implementation of a randomized algorithm for principal component
analysis
A. Szlam et al. 2014</li>
</ul>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.big_data.randomized.randomizedPinv">
<code class="descclassname">hyperlearn.big_data.randomized.</code><code class="descname">randomizedPinv</code><span class="sig-paren">(</span><em>X</em>, <em>n_components=None</em>, <em>alpha=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/big_data/randomized.html#randomizedPinv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.big_data.randomized.randomizedPinv" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 6/11/2018]
Implements fast randomized pseudoinverse with regularization.
Can be used as an approximation to the matrix inverse.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.big_data.randomized.randomizedSVD">
<code class="descclassname">hyperlearn.big_data.randomized.</code><code class="descname">randomizedSVD</code><span class="sig-paren">(</span><em>X</em>, <em>n_components=2</em>, <em>max_iter='auto'</em>, <em>solver='lu'</em>, <em>n_oversamples=10</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/big_data/randomized.html#randomizedSVD"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.big_data.randomized.randomizedSVD" title="Permalink to this definition">¶</a></dt>
<dd><p>[Edited 9/11/2018 Fixed SVD_flip]
HyperLearn’s Fast Randomized SVD is approx 10 - 30 % faster than
Sklearn’s implementation depending on n_components and max_iter.</p>
<p>Uses NUMBA Jit accelerated functions when available, and tries to
reduce memory overhead by chaining operations.</p>
<p>Uses QR, LU or no solver to find the best SVD decomp. QR is most stable,
but can be 2x slower than LU.</p>
<dl class="docutils">
<dt><a href="#id5"><span class="problematic" id="id6">**</span></a><a href="#id7"><span class="problematic" id="id8">**</span></a>n_oversamples = 10. This follows Sklearn convention to increase the chance</dt>
<dd>of more accurate SVD.</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li>Sklearn’s RandomizedSVD</li>
<li>Finding structure with randomness: Stochastic algorithms for constructing
approximate matrix decompositions
Halko, et al., 2009 <a class="reference external" href="http://arxiv.org/abs/arXiv:0909.4061">http://arxiv.org/abs/arXiv:0909.4061</a></li>
<li>A randomized algorithm for the decomposition of matrices
Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert</li>
<li>An implementation of a randomized algorithm for principal component
analysis
A. Szlam et al. 2014</li>
</ul>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.big_data.randomized.randomized_projection">
<code class="descclassname">hyperlearn.big_data.randomized.</code><code class="descname">randomized_projection</code><span class="sig-paren">(</span><em>X</em>, <em>k</em>, <em>solver='lu'</em>, <em>max_iter=4</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/big_data/randomized.html#randomized_projection"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.big_data.randomized.randomized_projection" title="Permalink to this definition">¶</a></dt>
<dd><p>[Edited 8/11/2018 Added QR Q_only parameter]
Projects X onto some random eigenvectors, then using a special
variant of Orthogonal Iteration, finds the closest orthogonal
representation for X.</p>
<p>Solver can be QR or LU or None.</p>
</dd></dl>

</div>
<div class="section" id="module-hyperlearn.big_data.truncated">
<span id="hyperlearn-big-data-truncated-module"></span><h2>hyperlearn.big_data.truncated module<a class="headerlink" href="#module-hyperlearn.big_data.truncated" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="hyperlearn.big_data.truncated.truncatedEigh">
<code class="descclassname">hyperlearn.big_data.truncated.</code><code class="descname">truncatedEigh</code><span class="sig-paren">(</span><em>XTX</em>, <em>n_components=2</em>, <em>tol=None</em>, <em>svd=False</em>, <em>which='largest'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/big_data/truncated.html#truncatedEigh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.big_data.truncated.truncatedEigh" title="Permalink to this definition">¶</a></dt>
<dd><p>[Edited 6/11/2018 Added smallest / largest command]
Computes the Truncated Eigendecomposition of a Hermitian Matrix
(positive definite). K = 2 for default.
Return format is LARGEST eigenvalue first.</p>
<p>If SVD is True, then outputs S2**0.5 and sets negative S2 to 0
and outputs VT and not V.</p>
<p>Uses ARPACK from Scipy to compute the truncated decomp. Note that
to make it slightly more stable and faster, follows Sklearn’s
random intialization from -1 -&gt; 1.</p>
<p>Also note tolerance is resolution(X), and NOT eps(X)</p>
<p>Might switch to SPECTRA in the future.</p>
<p>EIGH FLIP is called to flip the eigenvector signs for deterministic
output.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.big_data.truncated.truncatedSVD">
<code class="descclassname">hyperlearn.big_data.truncated.</code><code class="descname">truncatedSVD</code><span class="sig-paren">(</span><em>X</em>, <em>n_components=2</em>, <em>tol=None</em>, <em>transpose=True</em>, <em>U_decision=False</em>, <em>which='largest'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/big_data/truncated.html#truncatedSVD"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.big_data.truncated.truncatedSVD" title="Permalink to this definition">¶</a></dt>
<dd><p>[Edited 6/11/2018 Added which command - can get largest or smallest eigen components]
Computes the Truncated SVD of any matrix. K = 2 for default.
Return format is LARGEST singular first first.</p>
<p>Uses ARPACK from Scipy to compute the truncated decomp. Note that
to make it slightly more stable and faster, follows Sklearn’s
random intialization from -1 -&gt; 1.</p>
<p>Also note tolerance is resolution(X), and NOT eps(X). Also note
TRANSPOSE is True. This means instead of computing svd(X) if p &gt; n,
then computing svd(X.T) is faster, but you must output VT.T, S, U.T</p>
<p>Might switch to SPECTRA in the future.</p>
<p>SVD FLIP is called to flip the VT signs for deterministic
output. Note uses VT based decision and not U based decision.
U_decision can be changed to TRUE for Sklearn convention</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.big_data.truncated.truncatedEig">
<code class="descclassname">hyperlearn.big_data.truncated.</code><code class="descname">truncatedEig</code><span class="sig-paren">(</span><em>X</em>, <em>n_components=2</em>, <em>tol=None</em>, <em>svd=False</em>, <em>which='largest'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/big_data/truncated.html#truncatedEig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.big_data.truncated.truncatedEig" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 6/11/2018]
Computes truncated eigendecomposition given any matrix X. Directly
uses TruncatedSVD if memory is not enough, and returns eigen vectors/values.
Also argument for smallest eigen components are provided.</p>
</dd></dl>

</div>
<div class="section" id="module-hyperlearn.decomposition.base">
<span id="hyperlearn-decomposition-base-module"></span><h2>hyperlearn.decomposition.base module<a class="headerlink" href="#module-hyperlearn.decomposition.base" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="hyperlearn-decomposition-nmf-module">
<h2>hyperlearn.decomposition.NMF module<a class="headerlink" href="#hyperlearn-decomposition-nmf-module" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-hyperlearn.decomposition.PCA">
<span id="hyperlearn-decomposition-pca-module"></span><h2>hyperlearn.decomposition.PCA module<a class="headerlink" href="#module-hyperlearn.decomposition.PCA" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="hyperlearn.decomposition.PCA.PCA">
<em class="property">class </em><code class="descclassname">hyperlearn.decomposition.PCA.</code><code class="descname">PCA</code><span class="sig-paren">(</span><em>n_components=2</em>, <em>solver='eig'</em>, <em>alpha=None</em>, <em>fast=True</em>, <em>centre=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/decomposition/PCA.html#PCA"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.decomposition.PCA.PCA" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">hyperlearn.decomposition.base._basePCA</span></code></p>
<p>Principal Component Analysis reduces the dimensionality
of data by selecting the linear combinations of columns
or features which maximises the total variance.</p>
<p>Note, SKLEARN’s implementation is wasteful, as it uses
a full SVD solver, which is slow and painful.</p>
<p>HyperLearn’s implementation uses regularized Eigenvalue Decomposition,
but if solver = ‘svd’, then full SVD is used.</p>
<dl class="docutils">
<dt>If USE_GPU:</dt>
<dd>Uses PyTorch’s SVD (which is slow sadly), or EIGH. Speed is OK.</dd>
<dt>If CPU:</dt>
<dd>Uses Numpy’s Fortran C based SVD or EIGH.
If NUMBA is not installed, uses very fast LAPACK functions.</dd>
</dl>
<p>Alpha is added for regularization purposes. This prevents system
rounding errors and promises better convergence rates.</p>
<p>Note svd_flip is NOT same as SKLEARN, hence output may have reversed
signs. V based decision is used as EIGH is faster, and U is not computed.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.decomposition.PCA.diag">
<code class="descclassname">hyperlearn.decomposition.PCA.</code><code class="descname">diag</code><span class="sig-paren">(</span><em>input</em>, <em>diagonal=0</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#hyperlearn.decomposition.PCA.diag" title="Permalink to this definition">¶</a></dt>
<dd><ul class="simple">
<li>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a vector (1-D tensor), then returns a 2-D square tensor
with the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> as the diagonal.</li>
<li>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a matrix (2-D tensor), then returns a 1-D tensor with
the diagonal elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</li>
</ul>
<p>The argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">diagonal</span></code> controls which diagonal to consider:</p>
<ul class="simple">
<li>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">diagonal</span></code> = 0, it is the main diagonal.</li>
<li>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">diagonal</span></code> &gt; 0, it is above the main diagonal.</li>
<li>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">diagonal</span></code> &lt; 0, it is below the main diagonal.</li>
</ul>
<dl class="docutils">
<dt>Args:</dt>
<dd>input (Tensor): the input tensor
diagonal (int, optional): the diagonal to consider
out (Tensor, optional): the output tensor</dd>
</dl>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diagonal()</span></code> always returns the diagonal of its input.</p>
<p class="last"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diagflat()</span></code> always constructs a tensor with diagonal elements
specified by the input.</p>
</div>
<p>Examples:</p>
<p>Get the square matrix where the input vector is the diagonal:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.5950,-0.0872, 2.3298])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([[ 0.5950, 0.0000, 0.0000],</span>
<span class="go">        [ 0.0000,-0.0872, 0.0000],</span>
<span class="go">        [ 0.0000, 0.0000, 2.3298]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 0.0000, 0.5950, 0.0000, 0.0000],</span>
<span class="go">        [ 0.0000, 0.0000,-0.0872, 0.0000],</span>
<span class="go">        [ 0.0000, 0.0000, 0.0000, 2.3298],</span>
<span class="go">        [ 0.0000, 0.0000, 0.0000, 0.0000]])</span>
</pre></div>
</div>
<p>Get the k-th diagonal of a given matrix:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[-0.4264, 0.0255,-0.1064],</span>
<span class="go">        [ 0.8795,-0.2429, 0.1374],</span>
<span class="go">        [ 0.1029,-0.6482,-1.6300]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([-0.4264,-0.2429,-1.6300])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([ 0.0255, 0.1374])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.decomposition.PCA.ones">
<code class="descclassname">hyperlearn.decomposition.PCA.</code><code class="descname">ones</code><span class="sig-paren">(</span><em>*sizes</em>, <em>out=None</em>, <em>dtype=None</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#hyperlearn.decomposition.PCA.ones" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor filled with the scalar value <cite>1</cite>, with the shape defined
by the variable argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">sizes</span></code>.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first docutils">
<dt>sizes (int…): a sequence of integers defining the shape of the output tensor.</dt>
<dd>Can be a variable number of arguments or a collection like a list or tuple.</dd>
</dl>
<p>out (Tensor, optional): the output tensor
dtype (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional): the desired data type of returned tensor.</p>
<blockquote>
<div>Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code>).</div></blockquote>
<dl class="last docutils">
<dt>layout (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code>, optional): the desired layout of returned Tensor.</dt>
<dd>Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</dd>
<dt>device (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>, optional): the desired device of returned tensor.</dt>
<dd>Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</dd>
<dt>requires_grad (bool, optional): If autograd should record operations on the</dt>
<dd>returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</dd>
</dl>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  1.,  1.],</span>
<span class="go">        [ 1.,  1.,  1.]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="go">tensor([ 1.,  1.,  1.,  1.,  1.])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.decomposition.PCA.t_einsum">
<code class="descclassname">hyperlearn.decomposition.PCA.</code><code class="descname">t_einsum</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hyperlearn.decomposition.PCA.t_einsum" title="Permalink to this definition">¶</a></dt>
<dd><p>einsum(equation, operands) -&gt; Tensor</p>
<p>This function provides a way of computing multilinear expressions (i.e. sums of products) using the
Einstein summation convention.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>equation (string): The equation is given in terms of lower case letters (indices) to be associated</dt>
<dd>with each dimension of the operands and result. The left hand side lists the operands
dimensions, separated by commas. There should be one index letter per tensor dimension.
The right hand side follows after <cite>-&gt;</cite> and gives the indices for the output.
If the <cite>-&gt;</cite> and right hand side are omitted, it implicitly defined as the alphabetically
sorted list of all indices appearing exactly once in the left hand side.
The indices not apprearing in the output are summed over after multiplying the operands
entries.
If an index appears several times for the same operand, a diagonal is taken.
Ellipses <cite>…</cite> represent a fixed number of dimensions. If the right hand side is inferred,
the ellipsis dimensions are at the beginning of the output.</dd>
<dt>operands (list of Tensors): The operands to compute the Einstein sum of.</dt>
<dd>Note that the operands are passed as a list, not as individual arguments.</dd>
</dl>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;i,j-&gt;ij&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>  <span class="c1"># outer product</span>
<span class="go">tensor([[-0.0570, -0.0286, -0.0231,  0.0197],</span>
<span class="go">        [ 1.2616,  0.6335,  0.5113, -0.4351],</span>
<span class="go">        [ 1.4452,  0.7257,  0.5857, -0.4984],</span>
<span class="go">        [-0.4647, -0.2333, -0.1883,  0.1603],</span>
<span class="go">        [-1.1130, -0.5588, -0.4510,  0.3838]])</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bn,anm,bm-&gt;ba&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">l</span><span class="p">,</span><span class="n">A</span><span class="p">,</span><span class="n">r</span><span class="p">))</span> <span class="c1"># compare torch.nn.functional.bilinear</span>
<span class="go">tensor([[-0.3430, -5.2405,  0.4494],</span>
<span class="go">        [ 0.3311,  5.5201, -3.0356]])</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">As</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Bs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bij,bjk-&gt;bik&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">As</span><span class="p">,</span> <span class="n">Bs</span><span class="p">))</span> <span class="c1"># batch matrix multiplication</span>
<span class="go">tensor([[[-1.0564, -1.5904,  3.2023,  3.1271],</span>
<span class="go">         [-1.6706, -0.8097, -0.8025, -2.1183]],</span>

<span class="go">        [[ 4.2239,  0.3107, -0.5756, -0.2354],</span>
<span class="go">         [-1.4558, -0.3460,  1.5087, -0.8530]],</span>

<span class="go">        [[ 2.8153,  1.8787, -4.3839, -1.2112],</span>
<span class="go">         [ 0.3728, -2.1131,  0.0921,  0.8305]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ii-&gt;i&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">A</span><span class="p">,))</span> <span class="c1"># diagonal</span>
<span class="go">tensor([-0.7825,  0.8291, -0.1936])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;...ii-&gt;...i&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">A</span><span class="p">,))</span> <span class="c1"># batch diagonal</span>
<span class="go">tensor([[-1.0864,  0.7292,  0.0569],</span>
<span class="go">        [-0.9725, -1.0270,  0.6493],</span>
<span class="go">        [ 0.5832, -1.1716, -1.5084],</span>
<span class="go">        [ 0.4041, -1.1690,  0.8570]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;...ij-&gt;...ji&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">A</span><span class="p">,))</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># batch permute</span>
<span class="go">torch.Size([2, 3, 5, 4])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.decomposition.PCA.torch_dot">
<code class="descclassname">hyperlearn.decomposition.PCA.</code><code class="descname">torch_dot</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hyperlearn.decomposition.PCA.torch_dot" title="Permalink to this definition">¶</a></dt>
<dd><p>matmul(tensor1, tensor2, out=None) -&gt; Tensor</p>
<p>Matrix product of two tensors.</p>
<p>The behavior depends on the dimensionality of the tensors as follows:</p>
<ul class="simple">
<li>If both tensors are 1-dimensional, the dot product (scalar) is returned.</li>
<li>If both arguments are 2-dimensional, the matrix-matrix product is returned.</li>
<li>If the first argument is 1-dimensional and the second argument is 2-dimensional,
a 1 is prepended to its dimension for the purpose of the matrix multiply.
After the matrix multiply, the prepended dimension is removed.</li>
<li>If the first argument is 2-dimensional and the second argument is 1-dimensional,
the matrix-vector product is returned.</li>
<li>If both arguments are at least 1-dimensional and at least one argument is
N-dimensional (where N &gt; 2), then a batched matrix multiply is returned.  If the first
argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the
batched matrix multiply and removed after.  If the second argument is 1-dimensional, a
1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.
The non-matrix (i.e. batch) dimensions are <span class="xref std std-ref">broadcasted</span> (and thus
must be broadcastable).  For example, if <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor1</span></code> is a
<span class="math notranslate nohighlight">\((j \times 1 \times n \times m)\)</span> tensor and <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor2</span></code> is a <span class="math notranslate nohighlight">\((k \times m \times p)\)</span>
tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> will be an <span class="math notranslate nohighlight">\((j \times k \times n \times p)\)</span> tensor.</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The 1-dimensional dot product version of this function does not support an <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> parameter.</p>
</div>
<dl class="docutils">
<dt>Arguments:</dt>
<dd>tensor1 (Tensor): the first tensor to be multiplied
tensor2 (Tensor): the second tensor to be multiplied
out (Tensor, optional): the output tensor</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># vector x vector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># matrix x vector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># batched matrix x broadcasted vector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([10, 3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># batched matrix x batched matrix</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([10, 3, 5])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># batched matrix x broadcasted matrix</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([10, 3, 5])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="id9">
<h2>hyperlearn.decomposition.PCA module<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-hyperlearn.decomposition.PCA"></span><dl class="class">
<dt>
<em class="property">class </em><code class="descclassname">hyperlearn.decomposition.PCA.</code><code class="descname">PCA</code><span class="sig-paren">(</span><em>n_components=2</em>, <em>solver='eig'</em>, <em>alpha=None</em>, <em>fast=True</em>, <em>centre=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/decomposition/PCA.html#PCA"><span class="viewcode-link">[source]</span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">hyperlearn.decomposition.base._basePCA</span></code></p>
<p>Principal Component Analysis reduces the dimensionality
of data by selecting the linear combinations of columns
or features which maximises the total variance.</p>
<p>Note, SKLEARN’s implementation is wasteful, as it uses
a full SVD solver, which is slow and painful.</p>
<p>HyperLearn’s implementation uses regularized Eigenvalue Decomposition,
but if solver = ‘svd’, then full SVD is used.</p>
<dl class="docutils">
<dt>If USE_GPU:</dt>
<dd>Uses PyTorch’s SVD (which is slow sadly), or EIGH. Speed is OK.</dd>
<dt>If CPU:</dt>
<dd>Uses Numpy’s Fortran C based SVD or EIGH.
If NUMBA is not installed, uses very fast LAPACK functions.</dd>
</dl>
<p>Alpha is added for regularization purposes. This prevents system
rounding errors and promises better convergence rates.</p>
<p>Note svd_flip is NOT same as SKLEARN, hence output may have reversed
signs. V based decision is used as EIGH is faster, and U is not computed.</p>
</dd></dl>

<dl class="function">
<dt>
<code class="descclassname">hyperlearn.decomposition.PCA.</code><code class="descname">diag</code><span class="sig-paren">(</span><em>input</em>, <em>diagonal=0</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd><ul class="simple">
<li>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a vector (1-D tensor), then returns a 2-D square tensor
with the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> as the diagonal.</li>
<li>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a matrix (2-D tensor), then returns a 1-D tensor with
the diagonal elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</li>
</ul>
<p>The argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">diagonal</span></code> controls which diagonal to consider:</p>
<ul class="simple">
<li>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">diagonal</span></code> = 0, it is the main diagonal.</li>
<li>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">diagonal</span></code> &gt; 0, it is above the main diagonal.</li>
<li>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">diagonal</span></code> &lt; 0, it is below the main diagonal.</li>
</ul>
<dl class="docutils">
<dt>Args:</dt>
<dd>input (Tensor): the input tensor
diagonal (int, optional): the diagonal to consider
out (Tensor, optional): the output tensor</dd>
</dl>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diagonal()</span></code> always returns the diagonal of its input.</p>
<p class="last"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diagflat()</span></code> always constructs a tensor with diagonal elements
specified by the input.</p>
</div>
<p>Examples:</p>
<p>Get the square matrix where the input vector is the diagonal:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.5950,-0.0872, 2.3298])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([[ 0.5950, 0.0000, 0.0000],</span>
<span class="go">        [ 0.0000,-0.0872, 0.0000],</span>
<span class="go">        [ 0.0000, 0.0000, 2.3298]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 0.0000, 0.5950, 0.0000, 0.0000],</span>
<span class="go">        [ 0.0000, 0.0000,-0.0872, 0.0000],</span>
<span class="go">        [ 0.0000, 0.0000, 0.0000, 2.3298],</span>
<span class="go">        [ 0.0000, 0.0000, 0.0000, 0.0000]])</span>
</pre></div>
</div>
<p>Get the k-th diagonal of a given matrix:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[-0.4264, 0.0255,-0.1064],</span>
<span class="go">        [ 0.8795,-0.2429, 0.1374],</span>
<span class="go">        [ 0.1029,-0.6482,-1.6300]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([-0.4264,-0.2429,-1.6300])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([ 0.0255, 0.1374])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt>
<code class="descclassname">hyperlearn.decomposition.PCA.</code><code class="descname">ones</code><span class="sig-paren">(</span><em>*sizes</em>, <em>out=None</em>, <em>dtype=None</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd><p>Returns a tensor filled with the scalar value <cite>1</cite>, with the shape defined
by the variable argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">sizes</span></code>.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first docutils">
<dt>sizes (int…): a sequence of integers defining the shape of the output tensor.</dt>
<dd>Can be a variable number of arguments or a collection like a list or tuple.</dd>
</dl>
<p>out (Tensor, optional): the output tensor
dtype (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional): the desired data type of returned tensor.</p>
<blockquote>
<div>Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code>).</div></blockquote>
<dl class="last docutils">
<dt>layout (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code>, optional): the desired layout of returned Tensor.</dt>
<dd>Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</dd>
<dt>device (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>, optional): the desired device of returned tensor.</dt>
<dd>Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</dd>
<dt>requires_grad (bool, optional): If autograd should record operations on the</dt>
<dd>returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</dd>
</dl>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  1.,  1.],</span>
<span class="go">        [ 1.,  1.,  1.]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="go">tensor([ 1.,  1.,  1.,  1.,  1.])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt>
<code class="descclassname">hyperlearn.decomposition.PCA.</code><code class="descname">t_einsum</code><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>einsum(equation, operands) -&gt; Tensor</p>
<p>This function provides a way of computing multilinear expressions (i.e. sums of products) using the
Einstein summation convention.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><dl class="first last docutils">
<dt>equation (string): The equation is given in terms of lower case letters (indices) to be associated</dt>
<dd>with each dimension of the operands and result. The left hand side lists the operands
dimensions, separated by commas. There should be one index letter per tensor dimension.
The right hand side follows after <cite>-&gt;</cite> and gives the indices for the output.
If the <cite>-&gt;</cite> and right hand side are omitted, it implicitly defined as the alphabetically
sorted list of all indices appearing exactly once in the left hand side.
The indices not apprearing in the output are summed over after multiplying the operands
entries.
If an index appears several times for the same operand, a diagonal is taken.
Ellipses <cite>…</cite> represent a fixed number of dimensions. If the right hand side is inferred,
the ellipsis dimensions are at the beginning of the output.</dd>
<dt>operands (list of Tensors): The operands to compute the Einstein sum of.</dt>
<dd>Note that the operands are passed as a list, not as individual arguments.</dd>
</dl>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;i,j-&gt;ij&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>  <span class="c1"># outer product</span>
<span class="go">tensor([[-0.0570, -0.0286, -0.0231,  0.0197],</span>
<span class="go">        [ 1.2616,  0.6335,  0.5113, -0.4351],</span>
<span class="go">        [ 1.4452,  0.7257,  0.5857, -0.4984],</span>
<span class="go">        [-0.4647, -0.2333, -0.1883,  0.1603],</span>
<span class="go">        [-1.1130, -0.5588, -0.4510,  0.3838]])</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bn,anm,bm-&gt;ba&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">l</span><span class="p">,</span><span class="n">A</span><span class="p">,</span><span class="n">r</span><span class="p">))</span> <span class="c1"># compare torch.nn.functional.bilinear</span>
<span class="go">tensor([[-0.3430, -5.2405,  0.4494],</span>
<span class="go">        [ 0.3311,  5.5201, -3.0356]])</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">As</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Bs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bij,bjk-&gt;bik&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">As</span><span class="p">,</span> <span class="n">Bs</span><span class="p">))</span> <span class="c1"># batch matrix multiplication</span>
<span class="go">tensor([[[-1.0564, -1.5904,  3.2023,  3.1271],</span>
<span class="go">         [-1.6706, -0.8097, -0.8025, -2.1183]],</span>

<span class="go">        [[ 4.2239,  0.3107, -0.5756, -0.2354],</span>
<span class="go">         [-1.4558, -0.3460,  1.5087, -0.8530]],</span>

<span class="go">        [[ 2.8153,  1.8787, -4.3839, -1.2112],</span>
<span class="go">         [ 0.3728, -2.1131,  0.0921,  0.8305]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ii-&gt;i&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">A</span><span class="p">,))</span> <span class="c1"># diagonal</span>
<span class="go">tensor([-0.7825,  0.8291, -0.1936])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;...ii-&gt;...i&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">A</span><span class="p">,))</span> <span class="c1"># batch diagonal</span>
<span class="go">tensor([[-1.0864,  0.7292,  0.0569],</span>
<span class="go">        [-0.9725, -1.0270,  0.6493],</span>
<span class="go">        [ 0.5832, -1.1716, -1.5084],</span>
<span class="go">        [ 0.4041, -1.1690,  0.8570]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;...ij-&gt;...ji&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">A</span><span class="p">,))</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># batch permute</span>
<span class="go">torch.Size([2, 3, 5, 4])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt>
<code class="descclassname">hyperlearn.decomposition.PCA.</code><code class="descname">torch_dot</code><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>matmul(tensor1, tensor2, out=None) -&gt; Tensor</p>
<p>Matrix product of two tensors.</p>
<p>The behavior depends on the dimensionality of the tensors as follows:</p>
<ul class="simple">
<li>If both tensors are 1-dimensional, the dot product (scalar) is returned.</li>
<li>If both arguments are 2-dimensional, the matrix-matrix product is returned.</li>
<li>If the first argument is 1-dimensional and the second argument is 2-dimensional,
a 1 is prepended to its dimension for the purpose of the matrix multiply.
After the matrix multiply, the prepended dimension is removed.</li>
<li>If the first argument is 2-dimensional and the second argument is 1-dimensional,
the matrix-vector product is returned.</li>
<li>If both arguments are at least 1-dimensional and at least one argument is
N-dimensional (where N &gt; 2), then a batched matrix multiply is returned.  If the first
argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the
batched matrix multiply and removed after.  If the second argument is 1-dimensional, a
1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.
The non-matrix (i.e. batch) dimensions are <span class="xref std std-ref">broadcasted</span> (and thus
must be broadcastable).  For example, if <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor1</span></code> is a
<span class="math notranslate nohighlight">\((j \times 1 \times n \times m)\)</span> tensor and <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor2</span></code> is a <span class="math notranslate nohighlight">\((k \times m \times p)\)</span>
tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> will be an <span class="math notranslate nohighlight">\((j \times k \times n \times p)\)</span> tensor.</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The 1-dimensional dot product version of this function does not support an <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> parameter.</p>
</div>
<dl class="docutils">
<dt>Arguments:</dt>
<dd>tensor1 (Tensor): the first tensor to be multiplied
tensor2 (Tensor): the second tensor to be multiplied
out (Tensor, optional): the output tensor</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># vector x vector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># matrix x vector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># batched matrix x broadcasted vector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([10, 3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># batched matrix x batched matrix</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([10, 3, 5])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># batched matrix x broadcasted matrix</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([10, 3, 5])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="hyperlearn-discriminant-analysis-base-module">
<h2>hyperlearn.discriminant_analysis.base module<a class="headerlink" href="#hyperlearn-discriminant-analysis-base-module" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="hyperlearn-discriminant-analysis-qda-module">
<h2>hyperlearn.discriminant_analysis.QDA module<a class="headerlink" href="#hyperlearn-discriminant-analysis-qda-module" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-hyperlearn.impute.SVDImpute">
<span id="hyperlearn-impute-svdimpute-module"></span><h2>hyperlearn.impute.SVDImpute module<a class="headerlink" href="#module-hyperlearn.impute.SVDImpute" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="hyperlearn.impute.SVDImpute.fit">
<code class="descclassname">hyperlearn.impute.SVDImpute.</code><code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>n_components='auto'</em>, <em>standardise=True</em>, <em>copy=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/impute/SVDImpute.html#fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.impute.SVDImpute.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 31/10/2018] [Edited 2/11/2018 Fixed SVDImpute]</p>
<p>Fits a SVD onto the training data by projecting it to a lower space after
being intially filled with column means. By default, n_components is
determined automatically using log(p+1). Setting too low or too high mirrors
mean imputation, and deletes the purpose of SVD imputation.</p>
<p>Returns:
1. S    singular values
2. VT   eigenvectors
+ mean, std, mins</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.impute.SVDImpute.transform">
<code class="descclassname">hyperlearn.impute.SVDImpute.</code><code class="descname">transform</code><span class="sig-paren">(</span><em>X</em>, <em>S</em>, <em>VT</em>, <em>mean</em>, <em>std</em>, <em>mins</em>, <em>standardise</em>, <em>copy=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/impute/SVDImpute.html#transform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.impute.SVDImpute.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 31/10/2018] [Edited 2/11/2018 FIxed SVDImpute]</p>
<p>The fundamental advantage of HyperLearn’s SVD imputation is that a .transform
method is provided. I do not require seeing the whole matrix for imputation,
and can calculate SVD incrementally via the Incremental Module.</p>
</dd></dl>

</div>
<div class="section" id="module-hyperlearn.metrics.cosine">
<span id="hyperlearn-metrics-cosine-module"></span><h2>hyperlearn.metrics.cosine module<a class="headerlink" href="#module-hyperlearn.metrics.cosine" title="Permalink to this headline">¶</a></h2>
<dl class="attribute">
<dt id="hyperlearn.metrics.cosine.cosine_dis">
<code class="descclassname">hyperlearn.metrics.cosine.</code><code class="descname">cosine_dis</code><a class="reference internal" href="../_modules/hyperlearn/metrics/cosine.html#cosine_dis"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.metrics.cosine.cosine_dis" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 22/10/2018]
Performs XXT*-1 + 1 quickly on the lower triangular part.</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.metrics.cosine.cosine_dis_triangular">
<code class="descclassname">hyperlearn.metrics.cosine.</code><code class="descname">cosine_dis_triangular</code><a class="reference internal" href="../_modules/hyperlearn/metrics/cosine.html#cosine_dis_triangular"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.metrics.cosine.cosine_dis_triangular" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 22/10/2018]
Performs XXT*-1 + 1 quickly on the TCSR.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.metrics.cosine.cosine_distances">
<code class="descclassname">hyperlearn.metrics.cosine.</code><code class="descname">cosine_distances</code><span class="sig-paren">(</span><em>X</em>, <em>Y=None</em>, <em>triangular=False</em>, <em>n_jobs=1</em>, <em>copy=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/metrics/cosine.html#cosine_distances"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.metrics.cosine.cosine_distances" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 15/10/2018] [Edited 18/10/2018]
[Edited 22/10/2018 Added Y option]
Note: when using Y, speed improvement is approx 5-10% only from Sklearn.</p>
<p>Slightly faster than Sklearn’s Cosine Distances implementation.
If you set triangular to TRUE, the result is much much faster.
(Approx 50% faster than Sklearn)</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.metrics.cosine.cosine_distances_sparse">
<code class="descclassname">hyperlearn.metrics.cosine.</code><code class="descname">cosine_distances_sparse</code><span class="sig-paren">(</span><em>val</em>, <em>colPointer</em>, <em>rowIndices</em>, <em>n</em>, <em>p</em>, <em>triangular=False</em>, <em>dense_output=True</em>, <em>n_jobs=1</em>, <em>copy=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/metrics/cosine.html#cosine_distances_sparse"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.metrics.cosine.cosine_distances_sparse" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 22/10/2018]
Slightly faster than Sklearn’s Cosine Distances implementation.</p>
<p>If dense_output is set to FALSE, then a TCSR Matrix (Triangular CSR Matrix) is
provided and not a CSR matrix. This has the advantage of using only 1/2n^2 - n
memory and not n^2 memory.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.metrics.cosine.cosine_sim_triangular">
<code class="descclassname">hyperlearn.metrics.cosine.</code><code class="descname">cosine_sim_triangular</code><span class="sig-paren">(</span><em>N</em>, <em>D</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/metrics/cosine.html#cosine_sim_triangular"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.metrics.cosine.cosine_sim_triangular" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 21/10/2018]
Quickly performs X / norm_rows / norm_rows.T on the TCSR matrix.</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.metrics.cosine.cosine_sim_triangular_parallel">
<code class="descclassname">hyperlearn.metrics.cosine.</code><code class="descname">cosine_sim_triangular_parallel</code><a class="headerlink" href="#hyperlearn.metrics.cosine.cosine_sim_triangular_parallel" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 21/10/2018]
Quickly performs X / norm_rows / norm_rows.T on the TCSR matrix.</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.metrics.cosine.cosine_sim_triangular_single">
<code class="descclassname">hyperlearn.metrics.cosine.</code><code class="descname">cosine_sim_triangular_single</code><a class="headerlink" href="#hyperlearn.metrics.cosine.cosine_sim_triangular_single" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 21/10/2018]
Quickly performs X / norm_rows / norm_rows.T on the TCSR matrix.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.metrics.cosine.cosine_similarity">
<code class="descclassname">hyperlearn.metrics.cosine.</code><code class="descname">cosine_similarity</code><span class="sig-paren">(</span><em>X</em>, <em>Y=None</em>, <em>triangular=False</em>, <em>n_jobs=1</em>, <em>copy=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/metrics/cosine.html#cosine_similarity"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.metrics.cosine.cosine_similarity" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 20/10/2018] [Edited 22/201/2018]
[Edited 22/10/2018 Added Y option]
Note: when using Y, speed improvement is approx 5% only from Sklearn.</p>
<p>Cosine similarity is approx the same speed as Sklearn, but uses approx 10%
less memory. One clear advantage is if you set triangular to TRUE, then it’s faster.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.metrics.cosine.cosine_similarity_sparse">
<code class="descclassname">hyperlearn.metrics.cosine.</code><code class="descname">cosine_similarity_sparse</code><span class="sig-paren">(</span><em>val</em>, <em>colPointer</em>, <em>rowIndices</em>, <em>n</em>, <em>p</em>, <em>triangular=False</em>, <em>dense_output=True</em>, <em>n_jobs=1</em>, <em>copy=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/metrics/cosine.html#cosine_similarity_sparse"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.metrics.cosine.cosine_similarity_sparse" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 20/10/2018] [Edited 21/10/2018]
Slightly faster than Sklearn’s Cosine Similarity implementation.</p>
<p>If dense_output is set to FALSE, then a TCSR Matrix (Triangular CSR Matrix) is
provided and not a CSR matrix. This has the advantage of using only 1/2n^2 - n
memory and not n^2 memory.</p>
</dd></dl>

</div>
<div class="section" id="module-hyperlearn.metrics.euclidean">
<span id="hyperlearn-metrics-euclidean-module"></span><h2>hyperlearn.metrics.euclidean module<a class="headerlink" href="#module-hyperlearn.metrics.euclidean" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="hyperlearn.metrics.euclidean.euclidean_distances">
<code class="descclassname">hyperlearn.metrics.euclidean.</code><code class="descname">euclidean_distances</code><span class="sig-paren">(</span><em>X</em>, <em>Y=None</em>, <em>triangular=False</em>, <em>squared=False</em>, <em>n_jobs=1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/metrics/euclidean.html#euclidean_distances"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.metrics.euclidean.euclidean_distances" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 15/10/2018] [Edited 16/10/2018]
[Edited 22/10/2018 Added Y option]
Notice: parsing in Y will result in only 10% - 15% speed improvement, not 30%.</p>
<p>Much much faster than Sklearn’s implementation. Approx not 30% faster. Probably
even faster if using n_jobs = -1. Uses the idea that distance(X, X) is symmetric,
and thus algorithm runs only on 1/2 triangular part.</p>
<dl class="docutils">
<dt>Old complexity:</dt>
<dd><p class="first">X &#64; XT                  n^2p
rowSum(X^2)             np
XXT*-2                  n^2
XXT+X^2                 2n^2
maximum(XXT,0)  n^2</p>
<blockquote class="last">
<div>n^2p + 4n^2 + np</div></blockquote>
</dd>
<dt>New complexity:</dt>
<dd><p class="first">sym X &#64; XT              n^2p/2
rowSum(X^2)             np
sym XXT*-2              n^2/2
sym XXT+X^2             n^2
maximum(XXT,0)  n^2/2</p>
<blockquote class="last">
<div>n^2p/2 + 2n^2 + np</div></blockquote>
</dd>
</dl>
<p>So New complexity approx= 1/2(Old complexity)</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.metrics.euclidean.euclidean_distances_sparse">
<code class="descclassname">hyperlearn.metrics.euclidean.</code><code class="descname">euclidean_distances_sparse</code><span class="sig-paren">(</span><em>val</em>, <em>colPointer</em>, <em>rowIndices</em>, <em>n</em>, <em>p</em>, <em>triangular=False</em>, <em>dense_output=True</em>, <em>squared=False</em>, <em>n_jobs=1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/metrics/euclidean.html#euclidean_distances_sparse"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.metrics.euclidean.euclidean_distances_sparse" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 15/10/2018] [Edited 21/10/2018]
Much much faster than Sklearn’s implementation. Approx not 60% faster. Probably
even faster if using n_jobs = -1 (actually 73% faster). [n = 10,000 p = 1,000]
Uses the idea that distance(X, X) is symmetric, and thus algorithm runs only on
1/2 triangular part. Also notice memory usage is now 60% better than Sklearn.</p>
<p>If dense_output is set to FALSE, then a TCSR Matrix (Triangular CSR Matrix) is
provided and not a CSR matrix. This has the advantage of using only 1/2n^2 - n
memory and not n^2 memory.</p>
<dl class="docutils">
<dt>Old complexity:</dt>
<dd><p class="first">X &#64; XT                  n^2p
rowSum(X^2)             np
XXT*-2                  n^2
XXT+X^2                 2n^2
maximum(XXT,0)  n^2</p>
<blockquote class="last">
<div>n^2p + 4n^2 + np</div></blockquote>
</dd>
<dt>New complexity:</dt>
<dd><p class="first">sym X &#64; XT              n^2p/2
rowSum(X^2)             np
sym XXT*-2              n^2/2
sym XXT+X^2             n^2
maximum(XXT,0)  n^2/2</p>
<blockquote class="last">
<div>n^2p/2 + 2n^2 + np</div></blockquote>
</dd>
</dl>
<p>So New complexity approx= 1/2(Old complexity)</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.metrics.euclidean.euclidean_triangular">
<code class="descclassname">hyperlearn.metrics.euclidean.</code><code class="descname">euclidean_triangular</code><span class="sig-paren">(</span><em>S</em>, <em>D</em>, <em>squared=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/metrics/euclidean.html#euclidean_triangular"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.metrics.euclidean.euclidean_triangular" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 21/10/2018]
Quickly performs -2D + X^2 + X.T^2 on the TCSR matrix.
Also applies maximum(D, 0) and then square roots distances
if required.</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.metrics.euclidean.euclidean_triangular_parallel">
<code class="descclassname">hyperlearn.metrics.euclidean.</code><code class="descname">euclidean_triangular_parallel</code><a class="headerlink" href="#hyperlearn.metrics.euclidean.euclidean_triangular_parallel" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 21/10/2018]
Quickly performs -2D + X^2 + X.T^2 on the TCSR matrix.
Also applies maximum(D, 0) and then square roots distances
if required.</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.metrics.euclidean.euclidean_triangular_single">
<code class="descclassname">hyperlearn.metrics.euclidean.</code><code class="descname">euclidean_triangular_single</code><a class="headerlink" href="#hyperlearn.metrics.euclidean.euclidean_triangular_single" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 21/10/2018]
Quickly performs -2D + X^2 + X.T^2 on the TCSR matrix.
Also applies maximum(D, 0) and then square roots distances
if required.</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.metrics.euclidean.maximum0">
<code class="descclassname">hyperlearn.metrics.euclidean.</code><code class="descname">maximum0</code><a class="headerlink" href="#hyperlearn.metrics.euclidean.maximum0" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 15/10/2018] [Edited 21/10/2018]
Computes maxmimum(XXT, 0) faster. Much faster than Sklearn since uses
the notion that distance(X, X) is symmetric.</p>
<dl class="docutils">
<dt>Steps:</dt>
<dd><dl class="first last docutils">
<dt>maximum(XXT, 0)</dt>
<dd>Optimised. Instead of n^2 operations, does n(n-1)/2 operations.</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.metrics.euclidean.maximum0_parallel">
<code class="descclassname">hyperlearn.metrics.euclidean.</code><code class="descname">maximum0_parallel</code><a class="headerlink" href="#hyperlearn.metrics.euclidean.maximum0_parallel" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 15/10/2018] [Edited 21/10/2018]
Computes maxmimum(XXT, 0) faster. Much faster than Sklearn since uses
the notion that distance(X, X) is symmetric.</p>
<dl class="docutils">
<dt>Steps:</dt>
<dd><dl class="first last docutils">
<dt>maximum(XXT, 0)</dt>
<dd>Optimised. Instead of n^2 operations, does n(n-1)/2 operations.</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.metrics.euclidean.mult_minus2">
<code class="descclassname">hyperlearn.metrics.euclidean.</code><code class="descname">mult_minus2</code><a class="reference internal" href="../_modules/hyperlearn/metrics/euclidean.html#mult_minus2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.metrics.euclidean.mult_minus2" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 17/10/2018]
Quickly multiplies XXT by -2. Uses notion that XXT is symmetric,
hence only lower triangular is multiplied.</p>
</dd></dl>

</div>
<div class="section" id="module-hyperlearn.metrics.pairwise">
<span id="hyperlearn-metrics-pairwise-module"></span><h2>hyperlearn.metrics.pairwise module<a class="headerlink" href="#module-hyperlearn.metrics.pairwise" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-hyperlearn.sparse.base">
<span id="hyperlearn-sparse-base-module"></span><h2>hyperlearn.sparse.base module<a class="headerlink" href="#module-hyperlearn.sparse.base" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="hyperlearn.sparse.base.CreateCSR">
<code class="descclassname">hyperlearn.sparse.base.</code><code class="descname">CreateCSR</code><span class="sig-paren">(</span><em>X</em>, <em>n_jobs=1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/sparse/base.html#CreateCSR"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.base.CreateCSR" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 10/10/2018] [Edited 13/10/2018]
Much much faster than Scipy. In fact, HyperLearn uses less memory,
by noticing indices &gt;= 0, hence unsigned ints are used.</p>
<p>Likewise, parallelisation is seen possible with Numba with n_jobs.
Notice, an error message will be provided if 20% of the data is only zeros.
It needs to be more than 20% zeros for CSR Matrix to shine.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.sparse.base.create_csr">
<code class="descclassname">hyperlearn.sparse.base.</code><code class="descname">create_csr</code><span class="sig-paren">(</span><em>X</em>, <em>rowCount</em>, <em>nnz</em>, <em>temp</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/sparse/base.html#create_csr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.base.create_csr" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 10/10/2018] [Edited 13/10/2018]
Before used extra memory keeping a Boolean Matrix (np bytes) and a
ColIndex pointer which used p memory. Now, removed (np + p) memory usage,
meaning larger matrices can be handled.</p>
<p>Algorithm is 3 fold:</p>
<ol class="arabic simple">
<li>Create RowIndices</li>
<li><dl class="first docutils">
<dt>For every row in data:</dt>
<dd><ol class="first last arabic" start="3">
<li>Store until a non 0 is seen.</li>
</ol>
</dd>
</dl>
</li>
</ol>
<p>Algorithm takes approx O(n + np) time, which is similar to Scipy’s.
The only difference is now, parallelisation is possible, which can
cut the time to approx O(n + np/c) where c = no of threads</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.base.create_csr_cache">
<code class="descclassname">hyperlearn.sparse.base.</code><code class="descname">create_csr_cache</code><a class="headerlink" href="#hyperlearn.sparse.base.create_csr_cache" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 10/10/2018] [Edited 13/10/2018]
Before used extra memory keeping a Boolean Matrix (np bytes) and a
ColIndex pointer which used p memory. Now, removed (np + p) memory usage,
meaning larger matrices can be handled.</p>
<p>Algorithm is 3 fold:</p>
<ol class="arabic simple">
<li>Create RowIndices</li>
<li><dl class="first docutils">
<dt>For every row in data:</dt>
<dd><ol class="first last arabic" start="3">
<li>Store until a non 0 is seen.</li>
</ol>
</dd>
</dl>
</li>
</ol>
<p>Algorithm takes approx O(n + np) time, which is similar to Scipy’s.
The only difference is now, parallelisation is possible, which can
cut the time to approx O(n + np/c) where c = no of threads</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.base.create_csr_parallel">
<code class="descclassname">hyperlearn.sparse.base.</code><code class="descname">create_csr_parallel</code><a class="headerlink" href="#hyperlearn.sparse.base.create_csr_parallel" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 10/10/2018] [Edited 13/10/2018]
Before used extra memory keeping a Boolean Matrix (np bytes) and a
ColIndex pointer which used p memory. Now, removed (np + p) memory usage,
meaning larger matrices can be handled.</p>
<p>Algorithm is 3 fold:</p>
<ol class="arabic simple">
<li>Create RowIndices</li>
<li><dl class="first docutils">
<dt>For every row in data:</dt>
<dd><ol class="first last arabic" start="3">
<li>Store until a non 0 is seen.</li>
</ol>
</dd>
</dl>
</li>
</ol>
<p>Algorithm takes approx O(n + np) time, which is similar to Scipy’s.
The only difference is now, parallelisation is possible, which can
cut the time to approx O(n + np/c) where c = no of threads</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.base.determine_nnz">
<code class="descclassname">hyperlearn.sparse.base.</code><code class="descname">determine_nnz</code><a class="reference internal" href="../_modules/hyperlearn/sparse/base.html#determine_nnz"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.base.determine_nnz" title="Permalink to this definition">¶</a></dt>
<dd><p>Uses close to no memory at all when computing how many non
zeros are in the matrix. Notice the difference with Scipy
is HyperLearn does NOT use nonzero(). This reduces memory
usage dramatically.</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.sparse.base.getDtype">
<code class="descclassname">hyperlearn.sparse.base.</code><code class="descname">getDtype</code><span class="sig-paren">(</span><em>p</em>, <em>size</em>, <em>uint=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/sparse/base.html#getDtype"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.base.getDtype" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the exact best possible data type for CSR Matrix
creation.</p>
</dd></dl>

</div>
<div class="section" id="module-hyperlearn.sparse.csr">
<span id="hyperlearn-sparse-csr-module"></span><h2>hyperlearn.sparse.csr module<a class="headerlink" href="#module-hyperlearn.sparse.csr" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="hyperlearn.sparse.csr.XXT_sparse">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">XXT_sparse</code><span class="sig-paren">(</span><em>val</em>, <em>colPointer</em>, <em>rowIndices</em>, <em>n</em>, <em>p</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#XXT_sparse"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.XXT_sparse" title="Permalink to this definition">¶</a></dt>
<dd><p>See _XXT_sparse documentation.</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.add_0">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">add_0</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#add_0"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.add_0" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.add_1">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">add_1</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#add_1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.add_1" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.add_A">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">add_A</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#add_A"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.add_A" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.diagonal">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">diagonal</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#diagonal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.diagonal" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 10/10/2018] [Edited 13/10/2018]
Extracts the diagonal elements of a CSR Matrix. Note only gets
square diagonal (not off-diagonal). HyperLearn’s algorithm is faster
than Scipy’s, as it uses binary search, whilst Scipy uses Linear Search.</p>
<p>d = min(n, p)
HyperLearn = O(d log p)
Scipy = O(dp)</p>
</dd></dl>

<dl class="function">
<dt id="hyperlearn.sparse.csr.diagonal_add">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">diagonal_add</code><span class="sig-paren">(</span><em>val</em>, <em>colPointer</em>, <em>rowIndices</em>, <em>n</em>, <em>p</em>, <em>addon</em>, <em>copy=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#diagonal_add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.diagonal_add" title="Permalink to this definition">¶</a></dt>
<dd><p>See _diagonal_add documentation.</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.div_0">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">div_0</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#div_0"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.div_0" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.div_1">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">div_1</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#div_1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.div_1" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.div_A">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">div_A</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#div_A"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.div_A" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.get_element">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">get_element</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#get_element"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.get_element" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 14/10/2018]
Get A[i,j] element. HyperLearn’s algorithm is O(logp) complexity, which is much
better than Scipy’s O(p) complexity. HyperLearn uses binary search to find the
element.</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.matT_mat">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">matT_mat</code><a class="headerlink" href="#hyperlearn.sparse.csr.matT_mat" title="Permalink to this definition">¶</a></dt>
<dd><p>Added [14/10/2018]
A.T &#64; X is found where X is a dense matrix. Mostly the same as Scipy, albeit slightly faster.
The difference is now, HyperLearn is parallelized, which can reduce times by 1/2 or more.</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.matT_mat_parallel">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">matT_mat_parallel</code><a class="headerlink" href="#hyperlearn.sparse.csr.matT_mat_parallel" title="Permalink to this definition">¶</a></dt>
<dd><p>Added [14/10/2018]
A.T &#64; X is found where X is a dense matrix. Mostly the same as Scipy, albeit slightly faster.
The difference is now, HyperLearn is parallelized, which can reduce times by 1/2 or more.</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.matT_vec">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">matT_vec</code><a class="headerlink" href="#hyperlearn.sparse.csr.matT_vec" title="Permalink to this definition">¶</a></dt>
<dd><p>Added [13/10/2018]
X.T &#64; y is found. Notice how instead of converting CSR to CSC matrix, a direct
X.T &#64; y can be found. Same complexity as mat_vec(X, y). Also, HyperLearn is
parallelized, allowing for O(np/c) complexity.</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.matT_vec_parallel">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">matT_vec_parallel</code><a class="headerlink" href="#hyperlearn.sparse.csr.matT_vec_parallel" title="Permalink to this definition">¶</a></dt>
<dd><p>Added [13/10/2018]
X.T &#64; y is found. Notice how instead of converting CSR to CSC matrix, a direct
X.T &#64; y can be found. Same complexity as mat_vec(X, y). Also, HyperLearn is
parallelized, allowing for O(np/c) complexity.</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.mat_mat">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">mat_mat</code><a class="headerlink" href="#hyperlearn.sparse.csr.mat_mat" title="Permalink to this definition">¶</a></dt>
<dd><p>Added [14/10/2018]
A &#64; X is found where X is a dense matrix. Mostly the same as Scipy, albeit slightly faster.
The difference is now, HyperLearn is parallelized, which can reduce times by 1/2 or more.</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.mat_mat_parallel">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">mat_mat_parallel</code><a class="headerlink" href="#hyperlearn.sparse.csr.mat_mat_parallel" title="Permalink to this definition">¶</a></dt>
<dd><p>Added [14/10/2018]
A &#64; X is found where X is a dense matrix. Mostly the same as Scipy, albeit slightly faster.
The difference is now, HyperLearn is parallelized, which can reduce times by 1/2 or more.</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.mat_vec">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">mat_vec</code><a class="headerlink" href="#hyperlearn.sparse.csr.mat_vec" title="Permalink to this definition">¶</a></dt>
<dd><p>Added [13/10/2018]
X &#64; y is found. Scipy &amp; HyperLearn has similar speed. Notice, now
HyperLearn can be parallelised! This reduces complexity to approx
O(np/c) where c = no of threads / cores</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.mat_vec_parallel">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">mat_vec_parallel</code><a class="headerlink" href="#hyperlearn.sparse.csr.mat_vec_parallel" title="Permalink to this definition">¶</a></dt>
<dd><p>Added [13/10/2018]
X &#64; y is found. Scipy &amp; HyperLearn has similar speed. Notice, now
HyperLearn can be parallelised! This reduces complexity to approx
O(np/c) where c = no of threads / cores</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.max_0">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">max_0</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#max_0"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.max_0" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.max_1">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">max_1</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#max_1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.max_1" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.max_A">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">max_A</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#max_A"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.max_A" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.mean_0">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">mean_0</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#mean_0"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.mean_0" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.mean_1">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">mean_1</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#mean_1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.mean_1" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.mean_A">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">mean_A</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#mean_A"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.mean_A" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.min_0">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">min_0</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#min_0"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.min_0" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.min_1">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">min_1</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#min_1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.min_1" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.min_A">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">min_A</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#min_A"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.min_A" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.mult_0">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">mult_0</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#mult_0"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.mult_0" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.mult_1">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">mult_1</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#mult_1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.mult_1" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.mult_A">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">mult_A</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#mult_A"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.mult_A" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.rowSum">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">rowSum</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#rowSum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.rowSum" title="Permalink to this definition">¶</a></dt>
<dd><p>[Added 17/10/2018]
Computes rowSum**2 for sparse matrix efficiently, instead of using einsum</p>
</dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.sum_0">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">sum_0</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#sum_0"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.sum_0" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.sum_1">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">sum_1</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#sum_1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.sum_1" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="hyperlearn.sparse.csr.sum_A">
<code class="descclassname">hyperlearn.sparse.csr.</code><code class="descname">sum_A</code><a class="reference internal" href="../_modules/hyperlearn/sparse/csr.html#sum_A"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hyperlearn.sparse.csr.sum_A" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-hyperlearn.sparse.tcsr">
<span id="hyperlearn-sparse-tcsr-module"></span><h2>hyperlearn.sparse.tcsr module<a class="headerlink" href="#module-hyperlearn.sparse.tcsr" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-hyperlearn">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-hyperlearn" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../base.html" class="btn btn-neutral float-right" title="hyperlearn.base" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="modules.html" class="btn btn-neutral" title="hyperlearn" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Daniel Han-Chen

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>